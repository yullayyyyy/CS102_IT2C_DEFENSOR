"title","author","subject","abstract","date"
"privacy-respecting type error telemetry at scale","ben greenman (brown university, usa / university of utah, usa), alan jeffrey (roblox, usa), shriram krishnamurthi (brown university, usa), mitesh shah (roblox, usa)","programming languages","context: roblox studio lets millions of creators build interactive experiences by programming in a variant of lua called luau. the creators form a broad group, ranging from novices writing their first script to professional developers; thus, luau must support a wide audience. as part of its efforts to support all kinds of programmers, luau includes an optional, gradual type system and goes to great lengths to minimize false positive errors. inquiry: since luau is currently being used by many creators, we want to collect data to improve the language and, in particular, the type system. the standard way to collect data is to deploy client-side telemetry; however, we cannot scrape personal data or proprietary information, which means we cannot collect source code fragments, error messages, or even filepaths. the research questions are thus about how to conduct telemetry that is not invasive and obtain insights from it about type errors. approach: we designed and implemented a pseudonymized, randomly-sampling telemetry system for luau. telemetry records include a timestamp, a session id, a reason for sending, and a numeric summary of the most recent type analyses. this information lets us study type errors over time without revealing private data. we deployed the system in roblox studio during spring 2023 and collected over 1.5 million telemetry records from over 340,000 sessions. knowledge: we present several findings about luau, all of which suggest that telemetry is an effective way to study type error pragmatics. one of the less-surprising findings is that opt-in gradual types are unpopular: there is an 100x gap between the number of untyped luau sessions and the number of typed ones. one surprise is that the strict mode for type analysis is overly conservative about interactions with data assets. a reassuring finding is that type analysis rarely hits its internal limits on problem size. grounding: our findings are supported by a dataset of over 1.5 million telemetry records. the data and scripts for analyzing it are available in an artifact. importance: beyond the immediate benefits to luau, our findings about types and type errors have implications for adoption and ergonomics in other gradual languages such as typescript, elixir, and typed racket. our telemetry design is of broad interest, as it reports on type errors without revealing sensitive information.",2024-03-04
"application of neural ordinary differential equations for tokamak plasma dynamics analysis","zefang liu, weston m. stacey","plasma physics","in the quest for controlled thermonuclear fusion, tokamaks present complex challenges in understanding burning plasma dynamics. this study introduces a multi-region multi-timescale transport model, employing neural ordinary differential equations (neural odes) to simulate the intricate energy transfer processes within tokamaks. our methodology leverages neural odes for the numerical derivation of diffusivity parameters from diii-d tokamak experimental data, enabling the precise modeling of energy interactions between electrons and ions across various regions, including the core, edge, and scrape-off layer. these regions are conceptualized as distinct nodes, capturing the critical timescales of radiation and transport processes essential for efficient tokamak operation. validation against diii-d plasmas under various auxiliary heating conditions demonstrates the model's effectiveness, ultimately shedding light on ways to enhance tokamak performance with deep learning.",2024-03-03
"towards privacy-aware sign language translation at scale","phillip rust, bowen shi, skyler wang, necati cihan camgöz, jean maillard","computation and language","a major impediment to the advancement of sign language translation (slt) is data scarcity. much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. furthermore, scaling slt using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of slt technologies should account for. in this work, we propose a two-stage framework for privacy-aware slt at scale that addresses both of these issues. we introduce ssvp-slt, which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised slt finetuning on a curated parallel dataset. ssvp-slt achieves state-of-the-art finetuned and zero-shot gloss-free slt performance on the how2sign dataset, outperforming the strongest respective baselines by over 3 bleu-4. based on controlled experiments, we further discuss the advantages and limitations of self-supervised pretraining and anonymization via facial obfuscation for slt.",2024-02-14
"data efficiency and long term prediction capabilities for neural operator surrogate models of core and edge plasma codes","n. carey, l. zanisi, s. pamela, v. gopakumar, j. omotani, j. buchanan, j. brandstetter","plasma physics","simulation-based plasma scenario development, optimization and control are crucial elements towards the successful deployment of next-generation experimental tokamaks and fusion power plants. current simulation codes require extremely intensive use of hpc resources that make them unsuitable for iterative or real time applications. neural network based surrogate models of expensive simulators have been proposed to speed up such costly workflows. current efforts in this direction in the fusion community are mostly limited to point estimates of quantities of interest or simple 1d pde models, with a few notable exceptions. while the ai literature on methods for neural pde surrogate models is rich, performance benchmarks for fusion-relevant 2d fields has so far remained flimited. in this work neural pde surrogates are trained for the jorek mhd code and the storm scrape-off layer code using the pdearena library (this https url). the performance of these surrogate models is investigated as a function of training set size as well as for long-term predictions. the performance of surrogate models that are trained on either one variable or multiple variables at once is also considered. it is found that surrogates that are trained on more data perform best for both long- and short-term predictions. additionally, surrogate models trained on multiple variables achieve higher accuracy and more stable performance. downsampling the training set in time may provide stability in the long term at the expense of the short term predictive capability, but visual inspection of the resulting fields suggests that multiple metrics should be used to evaluate performance.",2024-02-13
"a step towards the integration of machine learning and small area estimation","tomasz żądło, adam chwila","methodology","the use of machine-learning techniques has grown in numerous research areas. currently, it is also widely used in statistics, including the official statistics for data collection (e.g. satellite imagery, web scraping and text mining, data cleaning, integration and imputation) but also for data analysis. however, the usage of these methods in survey sampling including small area estimation is still very limited. therefore, we propose a predictor supported by these algorithms which can be used to predict any population or subpopulation characteristics based on cross-sectional and longitudinal data. machine learning methods have already been shown to be very powerful in identifying and modelling complex and nonlinear relationships between the variables, which means that they have very good properties in case of strong departures from the classic assumptions. therefore, we analyse the performance of our proposal under a different set-up, in our opinion of greater importance in real-life surveys. we study only small departures from the assumed model, to show that our proposal is a good alternative in this case as well, even in comparison with optimal methods under the model. what is more, we propose the method of the accuracy estimation of machine learning predictors, giving the possibility of the accuracy comparison with classic methods, where the accuracy is measured as in survey sampling practice. the solution of this problem is indicated in the literature as one of the key issues in integration of these approaches. the simulation studies are based on a real, longitudinal dataset, freely available from the polish local data bank, where the prediction problem of subpopulation characteristics in the last period, with ""borrowing strength"" from other subpopulations and time periods, is considered.",2024-02-12
"scrapping the web for early wildfire detection","mateo lostanlen, felix veith, cristian buc, valentin barriere","computer vision and pattern recognition","early wildfire detection is of the utmost importance to enable rapid response efforts, and thus minimize the negative impacts of wildfire spreads. to this end, we present \pyro, a web-scraping-based dataset composed of videos of wildfires from a network of cameras that were enhanced with manual bounding-box-level annotations. our dataset was filtered based on a strategy to improve the quality and diversity of the data, reducing the final data to a set of 10,000 images. we ran experiments using a state-of-the-art object detection model and found out that the proposed dataset is challenging and its use in concordance with other public dataset helps to reach higher results overall. we will make our code and data publicly available.",2024-02-08
"going viral: an analysis of advertising of technology products on tiktok","ekansh agrawal","computers and society","social media has transformed the advertising landscape, becoming an essential tool for reaching and connecting with consumers. its sharing and engagement features amplify brand exposure, while its cost-effective options provide businesses with flexible advertising solutions. tiktok is a more recent social media platform that has gained popularity for advertising, particularly in the realm of e-commerce, due to its large user base and viral nature. tiktok had 1.2 billion monthly active users in q4 2021, generating an estimated $4.6 billion revenue in 2021. virality can lead to a massive increase in brand exposure, reaching a vast audience that may not have been accessible through traditional marketing efforts alone. advertisements for technological products are an example of such viral ads that are abundant on tiktok. the goal of this thesis is to understand how creators, community activity, and the recommendation algorithm influence the virality of advertisements for technology products on tiktok. the study analyzes various aspects of virality, including sentiment analysis, content characteristics, and the role of influencers. it employs data scraping and natural language processing tools to analyze metadata from 2,000 tiktok posts and 274,651, offering insights into the nuances of viral tech product advertising on tiktok.",2023-12-25
"rephrasing the web: a recipe for compute and data-efficient language modeling","pratyush maini, skyler seto, he bai, david grangier, yizhe zhang, navdeep jaitly","computation and language","large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. this is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. in this work, we propose web rephrase augmented pre-training ($\textbf{wrap}$) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as ""like wikipedia"" or in ""question-answer format"" to jointly pre-train llms on real and synthetic rephrases. first, we show that using wrap on the c4 dataset, which is naturally noisy, speeds up pre-training by $\sim3x$. at the same pre-training compute budget, it improves perplexity by more than 10% on average across different subsets of the pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%. second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of llms in ood settings. our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher 'quality' than web-scraped data.",2024-01-29
"designing and testing a mobile application for collecting whatsapp chat data while preserving privacy","brennan schaffner, archie brohn, jason chee, k.j. feng, marshini chetty","human-computer interaction","it is common practice for researchers to join public whatsapp chats and scrape their contents for analysis. however, research shows collecting data this way contradicts user expectations and preferences, even if the data is effectively public. to overcome these issues, we outline design considerations for collecting whatsapp chat data with improved user privacy by heightening user control and oversight of data collection and taking care to minimize the data researchers collect and process off a user's device. we refer to these design principles as user-centered data sharing (ucds). to evaluate our ucds principles, we implemented a mobile application representing one possible instance of these improved data collection techniques and evaluated the viability of using the app to collect whatsapp chat data. second, we surveyed whatsapp users to gather user perceptions on common existing whatsapp data collection methods as well as ucds methods. our results show that we were able to glean similar informative insights into whatsapp chats using ucds principles in our prototype app to common, less privacy-preserving methods. our survey showed that methods following the ucds principles are preferred by users because they offered users more control over the data collection process. future user studies could further expand upon ucds principles to overcome complications of researcher-to-group communication in research on whatsapp chats and evaluate these principles in other data sharing contexts.",2024-01-26
"double trouble? impact and detection of duplicates in face image datasets","torsten schlett, christian rathgeb, juan tapia, christoph busch","computer vision and pattern recognition","various face image datasets intended for facial biometrics research were created via web-scraping, i.e. the collection of images publicly available on the internet. this work presents an approach to detect both exactly and nearly identical face image duplicates, using file and image hashes. the approach is extended through the use of face image preprocessing. additional steps based on face recognition and face image quality assessment models reduce false positives, and facilitate the deduplication of the face images both for intra- and inter-subject duplicate sets. the presented approach is applied to five datasets, namely lfw, tinyface, adience, casia-webface, and c-ms-celeb (a cleaned ms-celeb-1m variant). duplicates are detected within every dataset, with hundreds to hundreds of thousands of duplicates for all except lfw. face recognition and quality assessment experiments indicate a minor impact on the results through the duplicate removal. the final deduplication data is publicly available.",2024-01-25
"memorization in self-supervised learning improves downstream generalization","wenhao wang, muhammad ahmad kaleem, adam dziedzic, michael backes, nicolas papernot, franziska boenisch","machine learning","self-supervised learning (ssl) has recently received significant attention due to its ability to train high-performance encoders purely on unlabeled data-often scraped from the internet. this data can still be sensitive and empirical evidence suggests that ssl encoders memorize private information of their training data and can disclose them at inference time. since existing theoretical definitions of memorization from supervised learning rely on labels, they do not transfer to ssl. to address this gap, we propose sslmem, a framework for defining memorization within ssl. our definition compares the difference in alignment of representations for data points and their augmented views returned by both encoders that were trained on these data points and encoders that were not. through comprehensive empirical analysis on diverse encoder architectures and datasets we highlight that even though ssl relies on large datasets and strong augmentations-both known in supervised learning as regularization techniques that reduce overfitting-still significant fractions of training data points experience high memorization. through our empirical results, we show that this memorization is essential for encoders to achieve higher generalization performance on different downstream tasks.",2024-01-19
"information retrieval and classification of real-time multi-source hurricane evacuation notices","tingting zhao, shubo tian, jordan daly, melissa geiger, minna jia, jinfeng zhang","information retrieval","for an approaching disaster, the tracking of time-sensitive critical information such as hurricane evacuation notices is challenging in the united states. these notices are issued and distributed rapidly by numerous local authorities that may spread across multiple states. they often undergo frequent updates and are distributed through diverse online portals lacking standard formats. in this study, we developed an approach to timely detect and track the locally issued hurricane evacuation notices. the text data were collected mainly with a spatially targeted web scraping method. they were manually labeled and then classified using natural language processing techniques with deep learning models. the classification of mandatory evacuation notices achieved a high accuracy (recall = 96%). we used hurricane ian (2022) to illustrate how real-time evacuation notices extracted from local government sources could be redistributed with a web gis system. our method applied to future hurricanes provides live data for situation awareness to higher-level government agencies and news media. the archived data helps scholars to study government responses toward weather warnings and individual behaviors influenced by evacuation history. the framework may be applied to other types of disasters for rapid and targeted retrieval, classification, redistribution, and archiving of real-time government orders and notifications.",2024-01-07
"a shocking amount of the web is machine translated: insights from multi-way parallelism","brian thompson, mehak preet dhaliwal, peter frisch, tobias domhan, marcello federico","computation and language","we show that content on the web is often translated into many languages, and the low quality of these multi-way translations indicates they were likely created using machine translation (mt). multi-way parallel, machine generated content not only dominates the translations in lower resource languages; it also constitutes a large fraction of the total web content in those languages. we also find evidence of a selection bias in the type of content which is translated into many languages, consistent with low quality english content being translated en masse into many lower resource languages, via mt. our work raises serious concerns about training models such as multilingual large language models on both monolingual and bilingual data scraped from the web.",2024-01-11
"examining the challenges in archiving instagram","rachel zheng, michele c. weigle","digital libraries","to prevent the spread of disinformation on instagram, we need to study the accounts and content of disinformation actors. however, due to their malicious nature, instagram often bans accounts that are responsible for spreading disinformation, making these accounts inaccessible from the live web. the only way we can study the content of banned accounts is through public web archives such as the internet archive. however, there are many issues present with archiving instagram pages. specifically, we focused on the issue that many wayback machine instagram mementos redirect to the instagram login page. in this study, we determined that mementos of instagram account pages on the wayback machine began redirecting to the instagram login page in august 2019. we also found that instagram mementos on archive.today, this http url, and this http url are also not well archived in terms of quantity and quality. moreover, we were unsuccessful in all our attempts to archive katy perry's instagram account page on archive.today, this http url, and conifer. although in the minority, replayable instagram mementos exist in public archives and contain valuable data for studying disinformation on instagram. with that in mind, we developed a python script to web scrape instagram mementos. as of august 2023, the python script can scrape wayback machine archives of instagram account pages between november 7, 2012 and june 8, 2018.",2024-01-04
"zero-shot active learning using self supervised learning","abhishek sinha, shreya singh","machine learning","deep learning algorithms are often said to be data hungry. the performance of such algorithms generally improve as more and more annotated data is fed into the model. while collecting unlabelled data is easier (as they can be scraped easily from the internet), annotating them is a tedious and expensive task. given a fixed budget available for data annotation, active learning helps selecting the best subset of data for annotation, such that the deep learning model when trained over that subset will have maximum generalization performance under this budget. in this work, we aim to propose a new active learning approach which is model agnostic as well as one doesn't require an iterative process. we aim to leverage self-supervised learnt features for the task of active learning. the benefit of self-supervised learning, is that one can get useful feature representation of the input data, without having any annotation.",2024-01-03
"map-reduce for multiprocessing large data and multi-threading for data scraping","zefeng qiu, prashanth umapathy, qingquan zhang, guanqun song, ting zhu","numerical analysis","this document is the final project report for our advanced operating system class. during this project, we mainly focused on applying multiprocessing and multi-threading technology to our whole project and utilized the map-reduce algorithm in our data cleaning and data analysis process. in general, our project can be divided into two components: data scraping and data processing, where the previous part was almost web wrangling with employing potential multiprocessing or multi-threading technology to speed up the whole process. and after we collect and scrape a large amount value of data as mentioned above, we can use them as input to implement data cleaning and data analysis, during this period, we take advantage of the map-reduce algorithm to increase efficiency.",2023-12-23
"traces of memorisation in large language models for code","ali al-kaswan, maliheh izadi, arie van deursen","cryptography and security","large language models have gained significant popularity because of their ability to generate human-like text and potential applications in various fields, such as software engineering. large language models for code are commonly trained on large unsanitised corpora of source code scraped from the internet. the content of these datasets is memorised and can be extracted by attackers with data extraction attacks. in this work, we explore memorisation in large language models for code and compare the rate of memorisation with large language models trained on natural language. we adopt an existing benchmark for natural language and construct a benchmark for code by identifying samples that are vulnerable to attack. we run both benchmarks against a variety of models, and perform a data extraction attack. we find that large language models for code are vulnerable to data extraction attacks, like their natural language counterparts. from the training data that was identified to be potentially extractable we were able to extract 47% from a codegen-mono-16b code completion model. we also observe that models memorise more, as their parameter count grows, and that their pre-training data are also vulnerable to attack. we also find that data carriers are memorised at a higher rate than regular code or documentation and that different model architectures memorise different samples. data leakage has severe outcomes, so we urge the research community to further investigate the extent of this phenomenon using a wider range of models and extraction techniques in order to build safeguards to mitigate this issue.",2023-12-18
"analyzing the impact of fake news on the anticipated outcome of the 2024 election ahead of time","shaina raza, mizanur rahman, shardul ghuge","computation and language","despite increasing awareness and research around fake news, there is still a significant need for datasets that specifically target racial slurs and biases within north american political speeches. this is particulary important in the context of upcoming north american elections. this study introduces a comprehensive dataset that illuminates these critical aspects of misinformation. to develop this fake news dataset, we scraped and built a corpus of 40,000 news articles about political discourses in north america. a portion of this dataset (4000) was then carefully annotated, using a blend of advanced language models and human verification methods. we have made both these datasets openly available to the research community and have conducted benchmarking on the annotated data to demonstrate its utility. we release the best-performing language model along with data. we encourage researchers and developers to make use of this dataset and contribute to this ongoing initiative.",2023-12-01
"automatic detection of problem-gambling signs from online texts using large language models","elke smith, nils reiter, jan peters","computation and language","problem gambling is a major public health concern and is associated with profound psychological distress and economic problems. there are numerous gambling communities on the internet where users exchange information about games, gambling tactics, as well as gambling-related problems. individuals exhibiting higher levels of problem gambling engage more in such communities. online gambling communities may provide insights into problem-gambling behaviour. using data scraped from a major german gambling discussion board, we fine-tuned a large language model, specifically a bidirectional encoder representations from transformers (bert) model, to predict signs of problem-gambling from forum posts. training data were generated by manual annotation and by taking into account diagnostic criteria and gambling-related cognitive distortions. using k-fold cross-validation, our models achieved a precision of 0.95 and f1 score of 0.71, demonstrating that satisfactory classification performance can be achieved by generating high-quality training material through manual annotation based on diagnostic criteria. the current study confirms that a bert-based model can be reliably used on small data sets and to detect signatures of problem gambling in online communication data. such computational approaches may have potential for the detection of changes in problem-gambling prevalence among online users.",2023-11-24
"reducr: robust data downsampling using class priority reweighting","william bankes, george hughes, ilija bogunovic, zi wang","machine learning","modern machine learning models are becoming increasingly expensive to train for real-world image and text classification tasks, where massive web-scale data is collected in a streaming fashion. to reduce the training cost, online batch selection techniques have been developed to choose the most informative datapoints. however, these techniques can suffer from poor worst-class generalization performance due to class imbalance and distributional shifts. this work introduces reducr, a robust and efficient data downsampling method that uses class priority reweighting. reducr reduces the training data while preserving worst-class generalization performance. reducr assigns priority weights to datapoints in a class-aware manner using an online learning algorithm. we demonstrate the data efficiency and robust performance of reducr on vision and text classification tasks. on web-scraped datasets with imbalanced class distributions, reducr significantly improves worst-class test accuracy (and average accuracy), surpassing state-of-the-art methods by around 15%.",2023-12-01
"universal backdoor attacks","benjamin schneider, nils lukas, florian kerschbaum","machine learning","web-scraped datasets are vulnerable to data poisoning, which can be used for backdooring deep image classifiers during training. since training on large datasets is expensive, a model is trained once and re-used many times. unlike adversarial examples, backdoor attacks often target specific classes rather than any class learned by the model. one might expect that targeting many classes through a naive composition of attacks vastly increases the number of poison samples. we show this is not necessarily true and more efficient, universal data poisoning attacks exist that allow controlling misclassifications from any source class into any target class with a small increase in poison samples. our idea is to generate triggers with salient characteristics that the model can learn. the triggers we craft exploit a phenomenon we call inter-class poison transferability, where learning a trigger from one class makes the model more vulnerable to learning triggers for other classes. we demonstrate the effectiveness and robustness of our universal backdoor attacks by controlling models with up to 6,000 classes while poisoning only 0.15% of the training dataset. our source code is available at this https url.",2023-11-30
"a large-scale longitudinal structured dataset of the dark web cryptomarket evolution (2014-2015)","hanjo d. boekhout, arjan a. blokland, frank w. takes","social and information networks","dark web marketplaces (dwm) facilitate the online trade of illicit goods. due to the illicit nature of these marketplaces, quality datasets are scarce and difficult to produce. the dark net market archives (2015) presented raw scraped source files crawled from a selection of dwms, including evolution. here, we present, specifically for the evolution dwm, a structured dataset extracted from dark net market archive data. uniquely, many of the data quality issues inherent to crawled data are resolved. the dataset covers over 500 thousand forum posts and over 80 thousand listings, providing data on forums, topics, posts, forum users, market vendors, listings, and more. additionally, we present temporal weighted communication networks extracted from this data. the presented dataset provides easy access to a high quality dwm dataset to facilitate the study of criminal behaviour and communication on such dwms, which may provide a relevant source of knowledge for researchers across disciplines, from social science to law to network science.",2023-11-20
"separating the wheat from the chaff with bread: an open-source benchmark and metrics to detect redundancy in text","isaac caswell, lisa wang, isabel papadimitriou","computation and language","data quality is a problem that perpetually resurfaces throughout the field of nlp, regardless of task, domain, or architecture, and remains especially severe for lower-resource languages. a typical and insidious issue, affecting both training data and model output, is data that is repetitive and dominated by linguistically uninteresting boilerplate, such as price catalogs or computer-generated log files. though this problem permeates many web-scraped corpora, there has yet to be a benchmark to test against, or a systematic study to find simple metrics that generalize across languages and agree with human judgements of data quality. in the present work, we create and release bread, a human-labeled benchmark on repetitive boilerplate vs. plausible linguistic content, spanning 360 languages. we release several baseline cred (character redundancy) scores along with it, and evaluate their effectiveness on bread. we hope that the community will use this resource to develop better filtering methods, and that our reference implementations of cred scores can become standard corpus evaluation tools, driving the development of cleaner language modeling corpora, especially in low-resource languages.",2023-11-11
"defending our privacy with backdoors","dominik hintersdorf, lukas struppek, daniel neider, kristian kersting","machine learning","the proliferation of large ai models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. one of the concerns is that adversaries can extract information about the training data using privacy attacks. unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. we propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names and faces of individuals from vision-language models by fine-tuning them for only a few minutes instead of re-training them from scratch. specifically, through strategic insertion of backdoors into text encoders, we align the embeddings of sensitive phrases with those of neutral terms-""a person"" instead of the person's actual name. for image encoders, we map embeddings of individuals to be removed from the model to a universal, anonymous embedding. our empirical results demonstrate the effectiveness of our backdoor-based defense on clip by assessing its performance using a specialized privacy attack for zero-shot classifiers. our approach provides not only a new ""dual-use"" perspective on backdoor attacks, but also presents a promising avenue to enhance the privacy of individuals within models trained on uncurated web-scraped data.",2023-10-12
"tag your fish in the broken net: a responsible web framework for protecting online privacy and copyright","dawen zhang, boming xia, yue liu, xiwei xu, thong hoang, zhenchang xing, mark staples, qinghua lu, liming zhu","networking and internet architecture","the world wide web, a ubiquitous source of information, serves as a primary resource for countless individuals, amassing a vast amount of data from global internet users. however, this online data, when scraped, indexed, and utilized for activities like web crawling, search engine indexing, and, notably, ai model training, often diverges from the original intent of its contributors. the ascent of generative ai has accentuated concerns surrounding data privacy and copyright infringement. regrettably, the web's current framework falls short in facilitating pivotal actions like consent withdrawal or data copyright claims. while some companies offer voluntary measures, such as crawler access restrictions, these often remain inaccessible to individual users. to empower online users to exercise their rights and enable companies to adhere to regulations, this paper introduces a user-controlled consent tagging framework for online data. it leverages the extensibility of http and html in conjunction with the decentralized nature of distributed ledger technology. with this framework, users have the ability to tag their online data at the time of transmission, and subsequently, they can track and request the withdrawal of consent for their data from the data holders. a proof-of-concept system is implemented, demonstrating the feasibility of the framework. this work holds significant potential for contributing to the reinforcement of user consent, privacy, and copyright on the modern internet and lays the groundwork for future insights into creating a more responsible and user-centric web ecosystem.",2023-10-11
"automating customer service using langchain: building custom open-source gpt chatbot for organizations","keivalya pandya, mehfuza holia","computation and language","in the digital age, the dynamics of customer service are evolving, driven by technological advancements and the integration of large language models (llms). this research paper introduces a groundbreaking approach to automating customer service using langchain, a custom llm tailored for organizations. the paper explores the obsolescence of traditional customer support techniques, particularly frequently asked questions (faqs), and proposes a paradigm shift towards responsive, context-aware, and personalized customer interactions. the heart of this innovation lies in the fusion of open-source methodologies, web scraping, fine-tuning, and the seamless integration of langchain into customer service platforms. this open-source state-of-the-art framework, presented as ""sahaay,"" demonstrates the ability to scale across industries and organizations, offering real-time support and query resolution. key elements of this research encompass data collection via web scraping, the role of embeddings, the utilization of google's flan t5 xxl, base and small language models for knowledge retrieval, and the integration of the chatbot into customer service platforms. the results section provides insights into their performance and use cases, here particularly within an educational institution. this research heralds a new era in customer service, where technology is harnessed to create efficient, personalized, and responsive interactions. sahaay, powered by langchain, redefines the customer-company relationship, elevating customer retention, value extraction, and brand image. as organizations embrace llms, customer service becomes a dynamic and customer-centric ecosystem.",2023-10-09
"leveraging unpaired data for vision-language generative models via cycle consistency","tianhong li, sangnie bhardwaj, yonglong tian, han zhang, jarred barber, dina katabi, guillaume lajoie, huiwen chang, dilip krishnan","computer vision and pattern recognition","current vision-language generative models rely on expansive corpora of paired image-text data to attain optimal performance and generalization capabilities. however, automatically collecting such data (e.g. via large-scale web scraping) leads to low quality and poor image-text correlation, while human annotation is more accurate but requires significant manual effort and expense. we introduce $\textbf{itit}$ ($\textbf{i}$n$\textbf{t}$egrating $\textbf{i}$mage $\textbf{t}$ext): an innovative training paradigm grounded in the concept of cycle consistency which allows vision-language training on unpaired image and text data. itit is comprised of a joint image-text encoder with disjoint image and text decoders that enable bidirectional image-to-text and text-to-image generation in a single framework. during training, itit leverages a small set of paired image-text data to ensure its output matches the input reasonably well in both directions. simultaneously, the model is also trained on much larger datasets containing only images or texts. this is achieved by enforcing cycle consistency between the original unpaired samples and the cycle-generated counterparts. for instance, it generates a caption for a given input image and then uses the caption to create an output image, and enforces similarity between the input and output images. our experiments show that itit with unpaired datasets exhibits similar scaling behavior as using high-quality paired data. we demonstrate image generation and captioning performance on par with state-of-the-art text-to-image and image-to-text models with orders of magnitude fewer (only 3m) paired image-text data.",2023-10-05
"web image formats: assessment of their real-world-usage and performance across popular web browsers","benedikt dornauer, michael felderer","performance","in 2023, images on the web make up 41% of transmitted data, significantly impacting the performance of web apps. fortunately, image formats like webp and avif could offer advanced compression and faster page loading, but may face performance disparities across browsers. therefore, we conducted performance evaluations on five major browsers - chrome, edge, safari, opera, and firefox - while comparing four image formats. the results indicate that the newer formats exhibited notable performance enhancements across all browsers, leading to shorter loading times. compared to the compressed jpeg format, webp and avif improved the page load time by 21% and 15%, respectively. however, web scraping revealed that jpeg and png still dominate web image choices, with webp at 4% as the most used new format. through the web scraping and web performance evaluation, this research serves to (1) explore image format preferences in web applications and analyze distribution and characteristics across frequently-visited sites in 2023 and (2) assess the performance impact of distinct web image formats on application load times across popular web browsers.",2023-10-01
"the strain on scientific publishing","mark a. hanson, pablo gómez barreiro, paolo crosetto, dan brockington","digital libraries","scientists are increasingly overwhelmed by the volume of articles being published. total articles indexed in scopus and web of science have grown exponentially in recent years; in 2022 the article total was 47% higher than in 2016, which has outpaced the limited growth, if any, in the number of practising scientists. thus, publication workload per scientist (writing, reviewing, editing) has increased dramatically. we define this problem as the strain on scientific publishing. to analyse this strain, we present five data-driven metrics showing publisher growth, processing times, and citation behaviours. we draw these data from web scrapes, requests for data from publishers, and material that is freely available through publisher websites. our findings are based on millions of papers produced by leading academic publishers. we find specific groups have disproportionately grown in their articles published per year, contributing to this strain. some publishers enabled this growth by adopting a strategy of hosting special issues, which publish articles with reduced turnaround times. given pressures on researchers to publish or perish to be competitive for funding applications, this strain was likely amplified by these offers to publish more articles. we also observed widespread year-over-year inflation of journal impact factors coinciding with this strain, which risks confusing quality signals. such exponential growth cannot be sustained. the metrics we define here should enable this evolving conversation to reach actionable solutions to address the strain on scientific publishing.",2023-09-27
"decorait -- decentralized opt-in/out registry for ai training","kar balan, alex black, simon jenni, andrew gilbert, andy parsons, john collomosse","cryptography and security","we present decorait; a decentralized registry through which content creators may assert their right to opt in or out of ai training as well as receive reward for their contributions. generative ai (genai) enables images to be synthesized using ai models trained on vast amounts of data scraped from public sources. model and content creators who may wish to share their work openly without sanctioning its use for training are thus presented with a data governance challenge. further, establishing the provenance of genai training data is important to creatives to ensure fair recognition and reward for their such use. we report a prototype of decorait, which explores hierarchical clustering and a combination of on/off-chain storage to create a scalable decentralized registry to trace the provenance of genai training data in order to determine training consent and reward creatives who contribute that data. decorait combines distributed ledger technology (dlt) with visual fingerprinting, leveraging the emerging c2pa (coalition for content provenance and authenticity) standard to create a secure, open registry through which creatives may express consent and data ownership for genai.",2023-09-25
"vidchapters-7m: video chapters at scale","antoine yang, arsha nagrani, ivan laptev, josef sivic, cordelia schmid","computer vision and pattern recognition","segmenting long videos into chapters enables users to quickly navigate to the information of their interest. this important topic has been understudied due to the lack of publicly released datasets. to address this issue, we present vidchapters-7m, a dataset of 817k user-chaptered videos including 7m chapters in total. vidchapters-7m is automatically created from videos online in a scalable manner by scraping user-annotated chapters and hence without any additional manual annotation. we introduce the following three tasks based on this data. first, the video chapter generation task consists of temporally segmenting the video and generating a chapter title for each segment. to further dissect the problem, we also define two variants of this task: video chapter generation given ground-truth boundaries, which requires generating a chapter title given an annotated video segment, and video chapter grounding, which requires temporally localizing a chapter given its annotated title. we benchmark both simple baselines and state-of-the-art video-language models for these three tasks. we also show that pretraining on vidchapters-7m transfers well to dense video captioning tasks in both zero-shot and finetuning settings, largely improving the state of the art on the youcook2 and vitt benchmarks. finally, our experiments reveal that downstream performance scales well with the size of the pretraining dataset. our dataset, code, and models are publicly available at this https url.",2023-09-25
"distilling adversarial prompts from safety benchmarks: report for the adversarial nibbler challenge","manuel brack, patrick schramowski, kristian kersting","computer vision and pattern recognition","text-conditioned image generation models have recently achieved astonishing image quality and alignment results. consequently, they are employed in a fast-growing number of applications. since they are highly data-driven, relying on billion-sized datasets randomly scraped from the web, they also produce unsafe content. as a contribution to the adversarial nibbler challenge, we distill a large set of over 1,000 potential adversarial inputs from existing safety benchmarks. our analysis of the gathered prompts and corresponding images demonstrates the fragility of input filters and provides further insights into systematic safety issues in current generative image models.",2023-09-20
"promap: datasets for product mapping in e-commerce","kateřina macková, martin pilát","machine learning","the goal of product mapping is to decide, whether two listings from two different e-shops describe the same products. existing datasets of matching and non-matching pairs of products, however, often suffer from incomplete product information or contain only very distant non-matching products. therefore, while predictive models trained on these datasets achieve good results on them, in practice, they are unusable as they cannot distinguish very similar but non-matching pairs of products. this paper introduces two new datasets for product mapping: promapcz consisting of 1,495 czech product pairs and promapen consisting of 1,555 english product pairs of matching and non-matching products manually scraped from two pairs of e-shops. the datasets contain both images and textual descriptions of the products, including their specifications, making them one of the most complete datasets for product mapping. additionally, the non-matching products were selected in two phases, creating two types of non-matches -- close non-matches and medium non-matches. even the medium non-matches are pairs of products that are much more similar than non-matches in other datasets -- for example, they still need to have the same brand and similar name and price. after simple data preprocessing, several machine learning algorithms were trained on these and two the other datasets to demonstrate the complexity and completeness of promap datasets. promap datasets are presented as a golden standard for further research of product mapping filling the gaps in existing ones.",2023-09-13
"unsupervised bias detection in college student newspapers","adam m. lehavi, william mccormack, noah kornfeld, solomon glazer","computation and language","this paper presents a pipeline with minimal human influence for scraping and detecting bias on college newspaper archives. this paper introduces a framework for scraping complex archive sites that automated tools fail to grab data from, and subsequently generates a dataset of 14 student papers with 23,154 entries. this data can also then be queried by keyword to calculate bias by comparing the sentiment of a large language model summary to the original article. the advantages of this approach are that it is less comparative than reconstruction bias and requires less labelled data than generating keyword sentiment. results are calculated on politically charged words as well as control words to show how conclusions can be drawn. the complete method facilitates the extraction of nuanced insights with minimal assumptions and categorizations, paving the way for a more objective understanding of bias within student newspaper sources.",2023-09-11
"when less is more: investigating data pruning for pretraining llms at scale","max marion, ahmet üstün, luiza pozzobon, alex wang, marzieh fadaee, sara hooker","computation and language","large volumes of text data have contributed significantly to the development of large language models (llms) in recent years. this data is typically acquired by scraping the internet, leading to pretraining datasets comprised of noisy web text. to date, efforts to prune these datasets down to a higher quality subset have relied on hand-crafted heuristics encoded as rule-based filters. in this work, we take a wider view and explore scalable estimates of data quality that can be used to systematically measure the quality of pretraining data. we perform a rigorous comparison at scale of the simple data quality estimator of perplexity, as well as more sophisticated and computationally intensive estimates of the error l2-norm and memorization. these metrics are used to rank and prune pretraining corpora, and we subsequently compare llms trained on these pruned datasets. surprisingly, we find that the simple technique of perplexity outperforms our more computationally expensive scoring methods. we improve over our no-pruning baseline while training on as little as 30% of the original training dataset. our work sets the foundation for unexplored strategies in automatically curating high quality corpora and suggests the majority of pretraining data can be removed while retaining performance.",2023-09-08
"npm-follower: a complete dataset tracking the npm ecosystem","donald pinckney, federico cassano, arjun guha, jonathan bell","software engineering","software developers typically rely upon a large network of dependencies to build their applications. for instance, the npm package repository contains over 3 million packages and serves tens of billions of downloads weekly. understanding the structure and nature of packages, dependencies, and published code requires datasets that provide researchers with easy access to metadata and code of packages. however, prior work on npm dataset construction typically has two limitations: 1) only metadata is scraped, and 2) packages or versions that are deleted from npm can not be scraped. over 330,000 versions of packages were deleted from npm between july 2022 and may 2023. this data is critical for researchers as it often pertains to important questions of security and malware. we present npm-follower, a dataset and crawling architecture which archives metadata and code of all packages and versions as they are published, and is thus able to retain data which is later deleted. the dataset currently includes over 35 million versions of packages, and grows at a rate of about 1 million versions per month. the dataset is designed to be easily used by researchers answering questions involving either metadata or program analysis. both the code and dataset are available at https://dependencies.science.",2023-08-24
"large language models as zero-shot conversational recommenders","zhankui he, zhouhang xie, rahul jha, harald steck, dawen liang, yesu feng, bodhisattwa prasad majumder, nathan kallus, julian mcauley","information retrieval","in this paper, we present empirical studies on conversational recommendation tasks using representative large language models in a zero-shot setting with three primary contributions. (1) data: to gain insights into model behavior in ""in-the-wild"" conversational recommendation scenarios, we construct a new dataset of recommendation-related conversations by scraping a popular discussion website. this is the largest public real-world conversational recommendation dataset to date. (2) evaluation: on the new dataset and two existing conversational recommendation datasets, we observe that even without fine-tuning, large language models can outperform existing fine-tuned conversational recommendation models. (3) analysis: we propose various probing tasks to investigate the mechanisms behind the remarkable performance of large language models in conversational recommendation. we analyze both the large language models' behaviors and the characteristics of the datasets, providing a holistic understanding of the models' effectiveness, limitations and suggesting directions for the design of future conversational recommenders",2023-08-19
"pug: photorealistic and semantically controllable synthetic data for representation learning","florian bordes, shashank shekhar, mark ibrahim, diane bouchacourt, pascal vincent, ari s. morcos","computer vision and pattern recognition","synthetic image datasets offer unmatched advantages for designing and evaluating deep neural networks: they make it possible to (i) render as many data samples as needed, (ii) precisely control each scene and yield granular ground truth labels (and captions), (iii) precisely control distribution shifts between training and testing to isolate variables of interest for sound experimentation. despite such promise, the use of synthetic image data is still limited -- and often played down -- mainly due to their lack of realism. most works therefore rely on datasets of real images, which have often been scraped from public images on the internet, and may have issues with regards to privacy, bias, and copyright, while offering little control over how objects precisely appear. in this work, we present a path to democratize the use of photorealistic synthetic data: we develop a new generation of interactive environments for representation learning research, that offer both controllability and realism. we use the unreal engine, a powerful game engine well known in the entertainment industry, to produce pug (photorealistic unreal graphics) environments and datasets for representation learning. in this paper, we demonstrate the potential of pug to enable more rigorous evaluations of vision models.",2023-08-08
"cream skimming the underground: identifying relevant information points from online forums","felipe moreno-vera, mateus nogueira, cainã figueiredo, daniel sadoc menasché, miguel bicudo, ashton woiwood, enrico lovat, anton kocheturov, leandro pfleger de aguiar","cryptography and security","this paper proposes a machine learning-based approach for detecting the exploitation of vulnerabilities in the wild by monitoring underground hacking forums. the increasing volume of posts discussing exploitation in the wild calls for an automatic approach to process threads and posts that will eventually trigger alarms depending on their content. to illustrate the proposed system, we use the crimebb dataset, which contains data scraped from multiple underground forums, and develop a supervised machine learning model that can filter threads citing cves and label them as proof-of-concept, weaponization, or exploitation. leveraging random forests, we indicate that accuracy, precision and recall above 0.99 are attainable for the classification task. additionally, we provide insights into the difference in nature between weaponization and exploitation, e.g., interpreting the output of a decision tree, and analyze the profits and other aspects related to the hacking communities. overall, our work sheds insight into the exploitation of vulnerabilities in the wild and can be used to provide additional ground truth to models such as epss and expected exploitability.",2023-08-03
"should we trust web-scraped data?","jens foerderer","general economics","the increasing adoption of econometric and machine-learning approaches by empirical researchers has led to a widespread use of one data collection method: web scraping. web scraping refers to the use of automated computer programs to access websites and download their content. the key argument of this paper is that naïve web scraping procedures can lead to sampling bias in the collected data. this article describes three sources of sampling bias in web-scraped data. more specifically, sampling bias emerges from web content being volatile (i.e., being subject to change), personalized (i.e., presented in response to request characteristics), and unindexed (i.e., abundance of a population register). in a series of examples, i illustrate the prevalence and magnitude of sampling bias. to support researchers and reviewers, this paper provides recommendations on anticipating, detecting, and overcoming sampling bias in web-scraped data.",2023-08-04
"specious sites: tracking the spread and sway of spurious news stories at scale","hans w. a. hanley, deepak kumar, zakir durumeric","social and information networks","misinformation, propaganda, and outright lies proliferate on the web, with some narratives having dangerous real-world consequences on public health, elections, and individual safety. however, despite the impact of misinformation, the research community largely lacks automated and programmatic approaches for tracking news narratives across online platforms. in this work, utilizing daily scrapes of 1,334 unreliable news websites, the large-language model mpnet, and dp-means clustering, we introduce a system to automatically identify and track the narratives spread within online ecosystems. identifying 52,036 narratives on these 1,334 websites, we describe the most prevalent narratives spread in 2022 and identify the most influential websites that originate and amplify narratives. finally, we show how our system can be utilized to detect new narratives originating from unreliable news websites and to aid fact-checkers in more quickly addressing misinformation. we release code and data at this https url.",2023-08-03
"improving multimodal datasets with image captioning","thao nguyen, samir yitzhak gadre, gabriel ilharco, sewoong oh, ludwig schmidt","machine learning","massive web datasets play a key role in the success of large vision-language models like clip and flamingo. however, the raw web data is noisy, and existing filtering methods to reduce noise often come at the expense of data diversity. our work focuses on caption quality as one major source of noise, and studies how generated captions can increase the utility of web-scraped datapoints with nondescript text. through exploring different mixing strategies for raw and generated captions, we outperform the best filtering method proposed by the datacomp benchmark by 2% on imagenet and 4% on average across 38 tasks, given a candidate pool of 128m image-text pairs. our best approach is also 2x better at flickr and ms-coco retrieval. we then analyze what makes synthetic captions an effective source of text supervision. in experimenting with different image captioning models, we also demonstrate that the performance of a model on standard image captioning benchmarks (e.g., nocaps cider) is not a reliable indicator of the utility of the captions it generates for multimodal training. finally, our experiments with using generated captions at datacomp's large scale (1.28b image-text pairs) offer insights into the limitations of synthetic text, as well as the importance of image curation with increasing training data quantity. the synthetic captions used in our experiments are now available on huggingface.",2023-07-19
"medical ministrations through web scraping","niketha sabesan, nivethitha, j.n shreyah, pranauv a j, shyam r","computation and language","web scraping is a technique that allows us to extract data from websites automatically. in the field of medicine, web scraping can be used to collect information about medical procedures, treatments, and healthcare providers. this information can be used to improve patient care, monitor the quality of healthcare services, and identify areas for improvement. one area where web scraping can be particularly useful is in medical ministrations. medical ministrations are the actions taken to provide medical care to patients, and web scraping can help healthcare providers identify the most effective ministrations for their patients. for example, healthcare providers can use web scraping to collect data about the symptoms and medical histories of their patients, and then use this information to determine the most appropriate ministrations. they can also use web scraping to gather information about the latest medical research and clinical trials, which can help them stay up-to-date with the latest treatments and procedures.",2023-06-21
"a responsive framework for research portals data using semantic web technology","muhammad zohaib","digital libraries","as the amount of data on the world wide web continues to grow exponentially, access to semantically structured information remains limited. the semantic web has emerged as a solution to enhance the machine-readability of data, making it significantly more accessible and interpretable. various techniques, such as web scraping and mapping, have been employed by different websites to provide semantic access. web scraping involves the extraction of valuable information from diverse data sources, such as the world wide web, utilizing powerful string manipulation this http url the research field, researchers face the challenge of collecting relevant data from multiple sources, which requires substantial time and effort. this research aims to address this issue by designing a framework for the semantic organization of research portal data. the framework focuses on the extraction of information from two specific research portals, namely microsoft academic and ieee xplore. its primary objective is to gather diverse research-related data from these targeted this http url implementing this framework, researchers can streamline the process of collecting valuable information for their work, saving time and effort. the semantic organization of research portal data offers enhanced accessibility and interpretability, facilitating more effective and efficient knowledge discovery. this research contributes to the advancement of research data management and promotes the utilization of semantic web technologies in the academic community.",2023-06-20
"web scraping: a promising tool for geographic data acquisition","alexander brenning, sebastian henn","information retrieval","with much of our lives taking place online, researchers are increasingly turning to information from the world wide web to gain insights into geographic patterns and processes. web scraping as an online data acquisition technique allows us to gather intelligence especially on social and economic actions for which the web serves as a platform. specific opportunities relate to near-real-time access to object-level geolocated data, which can be captured in a cost-effective way. the studied geographic phenomena include, but are not limited to, the rental market and associated processes such as gentrification, entrepreneurial ecosystems, or spatial planning processes. since the information retrieved from the web is not made available for that purpose, web scraping faces several unique challenges, several of which relate to location. ethical and legal issues mainly relate to intellectual property rights, informed consent and (geo-) privacy, and website integrity and contract. these issues also effect the practice of open science. in addition, there are technical and statistical challenges that relate to dependability and incompleteness, data inconsistencies and bias, as well as the limited historical coverage. geospatial analyses furthermore usually require the automated extraction and subsequent resolution of toponyms or addresses (geoparsing, geocoding). a study on apartment rent in leipzig, germany is used to illustrate the use of web scraping and its challenges. we conclude that geographic researchers should embrace web scraping as a powerful and affordable digital fieldwork tool while paying special attention to its legal, ethical, and methodological challenges.",2023-05-31
"what can we learn from unlearnable datasets?","pedro sandoval-segura, vasu singla, jonas geiping, micah goldblum, tom goldstein","machine learning","in an era of widespread web scraping, unlearnable dataset methods have the potential to protect data privacy by preventing deep neural networks from generalizing. but in addition to a number of practical limitations that make their use unlikely, we make a number of findings that call into question their ability to safeguard data. first, it is widely believed that neural networks trained on unlearnable datasets only learn shortcuts, simpler rules that are not useful for generalization. in contrast, we find that networks actually can learn useful features that can be reweighed for high test performance, suggesting that image protection is not assured. unlearnable datasets are also believed to induce learning shortcuts through linear separability of added perturbations. we provide a counterexample, demonstrating that linear separability of perturbations is not a necessary condition. to emphasize why linearly separable perturbations should not be relied upon, we propose an orthogonal projection attack which allows learning from unlearnable datasets published in icml 2021 and iclr 2023. our proposed attack is significantly less complex than recently proposed techniques.",2023-05-30
"mitigating inappropriateness in image generation: can there be value in reflecting the world's ugliness?","manuel brack, felix friedrich, patrick schramowski, kristian kersting","computer vision and pattern recognition","text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. since they are highly data-driven, relying on billion-sized datasets randomly scraped from the web, they also reproduce inappropriate human behavior. specifically, we demonstrate inappropriate degeneration on a large-scale for various generative text-to-image models, thus motivating the need for monitoring and moderating them at deployment. to this end, we evaluate mitigation strategies at inference to suppress the generation of inappropriate content. our findings show that we can use models' representations of the world's ugliness to align them with human preferences.",2023-05-28
"the curse of recursion: training on generated data makes models forget","ilia shumailov, zakhar shumaylov, yiren zhao, yarin gal, nicolas papernot, ross anderson","machine learning","stable diffusion revolutionised image creation from descriptive text. gpt-2, gpt-3(.5) and gpt-4 demonstrated astonishing performance across a variety of language tasks. chatgpt introduced such language models to the general public. it is now clear that large language models (llms) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. in this paper we consider what the future might hold. what will happen to gpt-{n} once llms contribute much of the language found online? we find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. we refer to this effect as model collapse and show that it can occur in variational autoencoders, gaussian mixture models and llms. we build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. we demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by llms in data crawled from the internet.",2023-05-27
"handling realistic label noise in bert text classification","maha tufail agro, hanan aldarmaki","computation and language","labels noise refers to errors in training labels caused by cheap data annotation methods, such as web scraping or crowd-sourcing, which can be detrimental to the performance of supervised classifiers. several methods have been proposed to counteract the effect of random label noise in supervised classification, and some studies have shown that bert is already robust against high rates of randomly injected label noise. however, real label noise is not random; rather, it is often correlated with input features or other annotator-specific factors. in this paper, we evaluate bert in the presence of two types of realistic label noise: feature-dependent label noise, and synthetic label noise from annotator disagreements. we show that the presence of these types of noise significantly degrades bert classification performance. to improve robustness, we evaluate different types of ensembles and noise-cleaning methods and compare their effectiveness against label noise across different datasets.",2023-05-23
"adversarial nibbler: a data-centric challenge for improving the safety of text-to-image models","alicia parrish, hannah rose kirk, jessica quaye, charvi rastogi, max bartolo, oana inel, juan ciro, rafael mosquera, addison howard, will cukierski, d. sculley, vijay janapa reddi, lora aroyo","machine learning","the generative ai revolution in recent years has been spurred by an expansion in compute power and data quantity, which together enable extensive pre-training of powerful text-to-image (t2i) models. with their greater capabilities to generate realistic and creative content, these t2i models like dall-e, midjourney, imagen or stable diffusion are reaching ever wider audiences. any unsafe behaviors inherited from pretraining on uncurated internet-scraped datasets thus have the potential to cause wide-reaching harm, for example, through generated images which are violent, sexually explicit, or contain biased and derogatory stereotypes. despite this risk of harm, we lack systematic and structured evaluation datasets to scrutinize model behavior, especially adversarial attacks that bypass existing safety filters. a typical bottleneck in safety evaluation is achieving a wide coverage of different types of challenging examples in the evaluation set, i.e., identifying 'unknown unknowns' or long-tail problems. to address this need, we introduce the adversarial nibbler challenge. the goal of this challenge is to crowdsource a diverse set of failure modes and reward challenge participants for successfully finding safety vulnerabilities in current state-of-the-art t2i models. ultimately, we aim to provide greater awareness of these issues and assist developers in improving the future safety and reliability of generative ai models. adversarial nibbler is a data-centric challenge, part of the dataperf challenge suite, organized and supported by kaggle and mlcommons.",2023-05-22
"on the relevance of apis facing fairwashed audits","jade garcia bourrée, erwan le merrer, gilles tredan, benoît rottembourg","machine learning","recent legislation required ai platforms to provide apis for regulators to assess their compliance with the law. research has nevertheless shown that platforms can manipulate their api answers through fairwashing. facing this threat for reliable auditing, this paper studies the benefits of the joint use of platform scraping and of apis. in this setup, we elaborate on the use of scraping to detect manipulated answers: since fairwashing only manipulates api answers, exploiting scraps may reveal a manipulation. to abstract the wide range of specific api-scrap situations, we introduce a notion of proxy that captures the consistency an auditor might expect between both data sources. if the regulator has a good proxy of the consistency, then she can easily detect manipulation and even bypass the api to conduct her audit. on the other hand, without a good proxy, relying on the api is necessary, and the auditor cannot defend against fairwashing. we then simulate practical scenarios in which the auditor may mostly rely on the api to conveniently conduct the audit task, while maintaining her chances to detect a potential manipulation. to highlight the tension between the audit task and the api fairwashing detection task, we identify pareto-optimal strategies in a practical audit scenario. we believe this research sets the stage for reliable audits in practical and manipulation-prone setups.",2023-05-23
"the dimensions of data labor: a road map for researchers, activists, and policymakers to empower data producers","hanlin li, nicholas vincent, stevie chancellor, brent hecht","computers and society","many recent technological advances (e.g. chatgpt and search engines) are possible only because of massive amounts of user-generated data produced through user interactions with computing systems or scraped from the web (e.g. behavior logs, user-generated content, and artwork). however, data producers have little say in what data is captured, how it is used, or who it benefits. organizations with the ability to access and process this data, e.g. openai and google, possess immense power in shaping the technology landscape. by synthesizing related literature that reconceptualizes the production of data for computing as ``data labor'', we outline opportunities for researchers, policymakers, and activists to empower data producers in their relationship with tech companies, e.g advocating for transparency about data reuse, creating feedback channels between data producers and companies, and potentially developing mechanisms to share data's revenue more broadly. in doing so, we characterize data labor with six important dimensions - legibility, end-use awareness, collaboration requirement, openness, replaceability, and livelihood overlap - based on the parallels between data labor and various other types of labor in the computing literature.",2023-05-22
"boosting distress support dialogue responses with motivational interviewing strategy","anuradha welivita, pearl pu","computation and language","ai-driven chatbots have become an emerging solution to address psychological distress. due to the lack of psychotherapeutic data, researchers use dialogues scraped from online peer support forums to train them. but since the responses in such platforms are not given by professionals, they contain both conforming and non-conforming responses. in this work, we attempt to recognize these conforming and non-conforming response types present in online distress-support dialogues using labels adapted from a well-established behavioral coding scheme named motivational interviewing treatment integrity (miti) code and show how some response types could be rephrased into a more mi adherent form that can, in turn, enable chatbot responses to be more compliant with the mi strategy. as a proof of concept, we build several rephrasers by fine-tuning blender and gpt3 to rephrase mi non-adherent ""advise without permission"" responses into ""advise with permission"". we show how this can be achieved with the construction of pseudo-parallel corpora avoiding costs for human labor. through automatic and human evaluation we show that in the presence of less training data, techniques such as prompting and data augmentation can be used to produce substantially good rephrasings that reflect the intended style and preserve the content of the original text.",2023-05-17
"pick your poison: undetectability versus robustness in data poisoning attacks","nils lukas, florian kerschbaum","cryptography and security","deep image classification models trained on vast amounts of web-scraped data are susceptible to data poisoning - a mechanism for backdooring models. a small number of poisoned samples seen during training can severely undermine a model's integrity during inference. existing work considers an effective defense as one that either (i) restores a model's integrity through repair or (ii) detects an attack. we argue that this approach overlooks a crucial trade-off: attackers can increase robustness at the expense of detectability (over-poisoning) or decrease detectability at the cost of robustness (under-poisoning). in practice, attacks should remain both undetectable and robust. detectable but robust attacks draw human attention and rigorous model evaluation or cause the model to be re-trained or discarded. in contrast, attacks that are undetectable but lack robustness can be repaired with minimal impact on model accuracy. our research points to intrinsic flaws in current attack evaluation methods and raises the bar for all data poisoning attackers who must delicately balance this trade-off to remain robust and undetectable. to demonstrate the existence of more potent defenders, we propose defenses designed to (i) detect or (ii) repair poisoned models using a limited amount of trusted image-label pairs. our results show that an attacker who needs to be robust and undetectable is substantially less threatening. our defenses mitigate all tested attacks with a maximum accuracy decline of 2% using only 1% of clean data on cifar-10 and 2.5% on imagenet. we demonstrate the scalability of our defenses by evaluating large vision-language models, such as clip. attackers who can manipulate the model's parameters pose an elevated risk as they can achieve higher robustness at low detectability compared to data poisoning attackers.",2023-05-07
"analysis of adversarial image manipulations","ahsi lo, gabriella pangelinan, michael c. king","computer vision and pattern recognition","as virtual and physical identity grow increasingly intertwined, the importance of privacy and security in the online sphere becomes paramount. in recent years, multiple news stories have emerged of private companies scraping web content and doing research with or selling the data. images uploaded online can be scraped without users' consent or knowledge. users of social media platforms whose images are scraped may be at risk of being identified in other uploaded images or in real-world identification situations. this paper investigates how simple, accessible image manipulation techniques affect the accuracy of facial recognition software in identifying an individual's various face images based on one unique image.",2023-05-10
"supernova: design strategies and opportunities for interactive visualization in computational notebooks","zijie j. wang, david munechika, seongmin lee, duen horng chau","human-computer interaction","computational notebooks such as jupyter notebook have become data scientists' de facto programming environments. many visualization researchers and practitioners have developed interactive visualization tools that support notebooks. however, little is known about the appropriate design of visual analytics (va) tools in notebooks. to bridge this critical research gap, we investigate the design strategies in this space by analyzing 159 notebook va tools and their users' feedback. our analysis encompasses 62 systems from academic papers and 103 systems sourced from a pool of 55k notebooks containing interactive visualizations that we obtain via scraping 8.6 million notebooks on github. we also examine findings from 15 user studies and user feedback in 379 github issues. through this work, we identify unique design opportunities and considerations for future notebook va tools, such as using and manipulating multimodal data in notebooks as well as balancing the degree of visualization-notebook integration. finally, we develop supernova, an open-source interactive tool to help researchers explore existing notebook va tools and search for related work.",2023-05-04
"the emotions of the crowd: learning image sentiment from tweets via cross-modal distillation","alessio serra, fabio carrara, maurizio tesconi, fabrizio falchi","computer vision and pattern recognition","trends and opinion mining in social media increasingly focus on novel interactions involving visual media, like images and short videos, in addition to text. in this work, we tackle the problem of visual sentiment analysis of social media images -- specifically, the prediction of image sentiment polarity. while previous work relied on manually labeled training sets, we propose an automated approach for building sentiment polarity classifiers based on a cross-modal distillation paradigm; starting from scraped multimodal (text + images) data, we train a student model on the visual modality based on the outputs of a textual teacher model that analyses the sentiment of the corresponding textual modality. we applied our method to randomly collected images crawled from twitter over three months and produced, after automatic cleaning, a weakly-labeled dataset of $\sim$1.5 million images. despite exploiting noisy labeled samples, our training pipeline produces classifiers showing strong generalization capabilities and outperforming the current state of the art on five manually labeled benchmarks for image sentiment polarity prediction.",2023-04-28
"social media in the global south: a network dataset of the malian twittersphere","daniel thilo schroeder, mirjam de bruijn, luca bruls, mulatu alemayehu moges, samba dialimpa badji, noëmie fritz, modibo galy cisse, johannes langguth, bruce mutsvairo, kristin skare orgeret","social and information networks","with the expansion of mobile communications infrastructure, social media usage in the global south is surging. compared to the global north, populations of the global south have had less prior experience with social media from stationary computers and wired internet. many countries are experiencing violent conflicts that have a profound effect on their societies. as a result, social networks develop under different conditions than elsewhere, and our goal is to provide data for studying this phenomenon. in this dataset paper, we present a data collection of a national twittersphere in a west african country of conflict. while not the largest social network in terms of users, twitter is an important platform where people engage in public discussion. the focus is on mali, a country beset by conflict since 2012 that has recently had a relatively precarious media ecology. the dataset consists of tweets and twitter users in mali and was collected in june 2022, when the malian conflict became more violent internally both towards external and international actors. in a preliminary analysis, we assume that the conflictual context influences how people access social media and, therefore, the shape of the twittersphere and its characteristics. the aim of this paper is to primarily invite researchers from various disciplines including complex networks and social sciences scholars to explore the data at hand further. we collected the dataset using a scraping strategy of the follower network and the identification of characteristics of a malian twitter user. the given snapshot of the malian twitter follower network contains around seven million accounts, of which 56,000 are clearly identifiable as malian. in addition, we present the tweets. the dataset is available at: this https url",2023-04-25
"along the margins: marginalized communities' ethical concerns about social platforms","lauren olson, emitzá guzmán, florian kunneman","software engineering","in this paper, we identified marginalized communities' ethical concerns about social platforms. we performed this identification because recent platform malfeasance indicates that software teams prioritize shareholder concerns over user concerns. additionally, these platform shortcomings often have devastating effects on marginalized populations. we first scraped 586 marginalized communities' subreddits, aggregated a dataset of their social platform mentions and manually annotated mentions of ethical concerns in these data. we subsequently analyzed trends in the manually annotated data and tested the extent to which ethical concerns can be automatically classified by means of natural language processing (nlp). we found that marginalized communities' ethical concerns predominantly revolve around discrimination and misrepresentation, and reveal deficiencies in current software development practices. as such, researchers and developers could use our work to further investigate these concerns and rectify current software flaws.",2023-04-18
"strongly intermittent far scrape-off layer fluctuations in alcator c-mod plasmas close to the empirical discharge density limit","sajidah ahmed, odd erik garcia, adam q kuang, brian labombard, james l terry, audun theodorsen","plasma physics","intermittent plasma fluctuations in the boundary region of the alcator c-mod device were comprehensively investigated using data time-series from gas puff imaging and mirror langmuir probe diagnostics. fluctuations were sampled during stationary plasma conditions in ohmically heated, lower single null diverted configurations with scans in both line-averaged density and plasma current, with greenwald density fractions up to 0.85. utilizing a stochastic model, we describe the plasma fluctuations as a super-position of uncorrelated pulses, with large-amplitude events corresponding to blob-like filaments moving through the scrape-off layer. a deconvolution method is used to estimate the pulse arrival times and amplitudes. the analysis reveals a significant increase of pulse amplitudes and waiting times as the line-averaged density approaches the empirical discharge density limit. broadened and flattened average radial profiles are thus accompanied by strongly intermittent and large-amplitude fluctuations. although these filaments are arriving less frequently at high line-averaged densities, we show that there are significant increases in radial far-sol particle and heat fluxes which will further enhance plasma--wall interactions. the stochastic model has been used as a framework for study of the scalings in the intermittency parameter, flux and mean amplitude and waiting times, and is being used to inform predictive capability for the effects of filamentary transport as a function of greenwald fraction.",2023-04-13
"zero-shot in-distribution detection in multi-object settings using vision-language foundation models","atsuyuki miyai, qing yu, go irie, kiyoharu aizawa","computer vision and pattern recognition","extracting in-distribution (id) images from noisy images scraped from the internet is an important preprocessing for constructing datasets, which has traditionally been done manually. automating this preprocessing with deep learning techniques presents two key challenges. first, images should be collected using only the name of the id class without training on the id data. second, as we can see why coco was created, it is crucial to identify images containing not only id objects but also both id and out-of-distribution (ood) objects as id images to create robust recognizers. in this paper, we propose a novel problem setting called zero-shot in-distribution (id) detection, where we identify images containing id objects as id images (even if they contain ood objects), and images lacking id objects as ood images without any training. to solve this problem, we leverage the powerful zero-shot capability of clip and present a simple and effective approach, global-local maximum concept matching (gl-mcm), based on both global and local visual-text alignments of clip features. extensive experiments demonstrate that gl-mcm outperforms comparison methods on both multi-object datasets and single-object imagenet benchmarks. the code will be available via this https url.",2023-04-10
"discriminative class tokens for text-to-image diffusion models","idan schwartz, vésteinn snæbjarnarson, hila chefer, ryan cotterell, serge belongie, lior wolf, sagie benaim","computer vision and pattern recognition","recent advances in text-to-image diffusion models have enabled the generation of diverse and high-quality images. while impressive, the images often fall short of depicting subtle details and are susceptible to errors due to ambiguity in the input text. one way of alleviating these issues is to train diffusion models on class-labeled datasets. this approach has two disadvantages: (i) supervised datasets are generally small compared to large-scale scraped text-image datasets on which text-to-image models are trained, affecting the quality and diversity of the generated images, or (ii) the input is a hard-coded label, as opposed to free-form text, limiting the control over the generated images. in this work, we propose a non-invasive fine-tuning technique that capitalizes on the expressive potential of free-form text while achieving high accuracy through discriminative signals from a pretrained classifier. this is done by iteratively modifying the embedding of an added input token of a text-to-image diffusion model, by steering generated images toward a given target class according to a classifier. our method is fast compared to prior fine-tuning methods and does not require a collection of in-class images or retraining of a noise-tolerant classifier. we evaluate our method extensively, showing that the generated images are: (i) more accurate and of higher quality than standard diffusion models, (ii) can be used to augment training data in a low-resource setting, and (iii) reveal information about the data used to train the guiding classifier. the code is available at \url{this https url}.",2023-03-30
"effects of extending residencies on the supply and quality of family medicine practitioners; difference-in-differences evidence from the implementation of mandatory family medicine residencies in canada","stephenson strobel","general economics","i examine the impacts of extending residency training programs on the supply and quality of physicians practicing primary care. i leverage mandated extended residency lengths for primary care practitioners that were rolled out over 20 years in canada on a province-by-province basis. i compare these primary care specialties to other specialties that did not change residency length (first difference) before and after the policy implementation (second difference) to assess how physician supply evolved in response. to examine quality outcomes, i use a set of scraped data and repeat this difference-in-differences identification strategy for complaints resulting in censure against physicians in ontario. i find declines in the number of primary care providers by 5% for up to nine years after the policy change. these changes are particularly pronounced in new grads and younger physicians suggesting that the policy change dissuaded these physicians from entering primary care residencies. i find no impacts on quality of physician as measured by public censure of physicians. this suggests that extending primary care training caused declines in physician supply without any concomitant improvement in the quality of these physicians. this has implications for current plans to extend residency training programs.",2023-03-24
"reclaiming the digital commons: a public data trust for training data","alan chan, herbie bradley, nitarshan rajkumar","computers and society","democratization of ai means not only that people can freely use ai, but also that people can collectively decide how ai is to be used. in particular, collective decision-making power is required to redress the negative externalities from the development of increasingly advanced ai systems, including degradation of the digital commons and unemployment from automation. the rapid pace of ai development and deployment currently leaves little room for this power. monopolized in the hands of private corporations, the development of the most capable foundation models has proceeded largely without public input. there is currently no implemented mechanism for ensuring that the economic value generated by such models is redistributed to account for their negative externalities. the citizens that have generated the data necessary to train models do not have input on how their data are to be used. in this work, we propose that a public data trust assert control over training data for foundation models. in particular, this trust should scrape the internet as a digital commons, to license to commercial model developers for a percentage cut of revenues from deployment. first, we argue in detail for the existence of such a trust. we also discuss feasibility and potential risks. second, we detail a number of ways for a data trust to incentivize model developers to use training data only from the trust. we propose a mix of verification mechanisms, potential regulatory action, and positive incentives. we conclude by highlighting other potential benefits of our proposed data trust and connecting our work to ongoing efforts in data and compute governance.",2023-03-16
"combining generative artificial intelligence (ai) and the internet: heading towards evolution or degradation?","gonzalo martínez, lauren watson, pedro reviriego, josé alberto hernández, marc juarez, rik sarkar","computer vision and pattern recognition","in the span of a few months, generative artificial intelligence (ai) tools that can generate realistic images or text have taken the internet by storm, making them one of the technologies with fastest adoption ever. some of these generative ai tools such as dall-e, midjourney, or chatgpt have gained wide public notoriety. interestingly, these tools are possible because of the massive amount of data (text and images) available on the internet. the tools are trained on massive data sets that are scraped from internet sites. and now, these generative ai tools are creating massive amounts of new data that are being fed into the internet. therefore, future versions of generative ai tools will be trained with internet data that is a mix of original and ai-generated data. as time goes on, a mixture of original data and data generated by different versions of ai tools will populate the internet. this raises a few intriguing questions: how will future versions of generative ai tools behave when trained on a mixture of real and ai generated data? will they evolve with the new data sets or degenerate? will evolution introduce biases in subsequent generations of generative ai tools? in this document, we explore these questions and report some very initial simulation results using a simple image-generation ai tool. these results suggest that the quality of the generated images degrades as more ai-generated data is used for training thus suggesting that generative ai may degenerate. although these results are preliminary and cannot be generalised without further study, they serve to illustrate the potential issues of the interaction between generative ai and the internet.",2023-02-17
"fair diffusion: instructing text-to-image generation models on fairness","felix friedrich, manuel brack, lukas struppek, dominik hintersdorf, patrick schramowski, sasha luccioni, kristian kersting","machine learning","generative ai models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. however, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. in fact, they may even reinforce such biases. to not only uncover but also combat these undesired effects, we present a novel strategy, called fair diffusion, to attenuate biases after the deployment of generative text-to-image models. specifically, we demonstrate shifting a bias, based on human instructions, in any direction yielding arbitrarily new proportions for, e.g., identity groups. as our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, with no data filtering and additional training required.",2023-02-07
"nl2cmd: an updated workflow for natural language to bash commands translation","quchen fu, zhongwei teng, marco georgaklis, jules white, douglas c. schmidt","computation and language","translating natural language into bash commands is an emerging research field that has gained attention in recent years. most efforts have focused on producing more accurate translation models. to the best of our knowledge, only two datasets are available, with one based on the other. both datasets involve scraping through known data sources (through platforms like stack overflow, crowdsourcing, etc.) and hiring experts to validate and correct either the english text or bash commands. this paper provides two contributions to research on synthesizing bash commands from scratch. first, we describe a state-of-the-art translation model used to generate bash commands from the corresponding english text. second, we introduce a new nl2cmd dataset that is automatically generated, involves minimal human intervention, and is over six times larger than prior datasets. since the generation pipeline does not rely on existing bash commands, the distribution and types of commands can be custom adjusted. we evaluate the performance of chatgpt on this task and discuss the potential of using it as a data generator. our empirical results show how the scale and diversity of our dataset can offer unique opportunities for semantic parsing researchers.",2023-02-15
"genaug: retargeting behaviors to unseen situations via generative augmentation","zoey chen, sho kiami, abhishek gupta, vikash kumar","robotics","robot learning methods have the potential for widespread generalization across tasks, environments, and objects. however, these methods require large diverse datasets that are expensive to collect in real-world robotics settings. for robot learning to generalize, we must be able to leverage sources of data or priors beyond the robot's own experience. in this work, we posit that image-text generative models, which are pre-trained on large corpora of web-scraped data, can serve as such a data source. we show that despite these generative models being trained on largely non-robotics data, they can serve as effective ways to impart priors into the process of robot learning in a way that enables widespread generalization. in particular, we show how pre-trained generative models can serve as effective tools for semantically meaningful data augmentation. by leveraging these pre-trained models for generating appropriate ""semantic"" data augmentations, we propose a system genaug that is able to significantly improve policy generalization. we apply genaug to tabletop manipulation tasks, showing the ability to re-target behavior to novel scenarios, while only requiring marginal amounts of real-world data. we demonstrate the efficacy of this system on a number of object manipulation problems in the real world, showing a 40% improvement in generalization to novel scenes and objects.",2023-02-13
"ethical considerations for responsible data curation","jerone t. a. andrews, dora zhao, william thong, apostolos modas, orestis papakyriakopoulos, alice xiang","computer vision and pattern recognition","human-centric computer vision (hccv) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. hccv datasets constructed through nonconsensual web scraping lack crucial metadata for comprehensive fairness and robustness evaluations. current remedies are post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. our research focuses on proactive, domain-specific recommendations, covering purpose, privacy and consent, and diversity, for curating hccv evaluation datasets, addressing privacy and bias concerns. we adopt an ante hoc reflective perspective, drawing from current practices, guidelines, dataset withdrawals, and audits, to inform our considerations and recommendations.",2023-02-07
"leaving reality to imagination: robust classification via generated datasets","hritik bansal, aditya grover","computer vision and pattern recognition","recent research on robustness has revealed significant performance gaps between neural image classifiers trained on datasets that are similar to the test set, and those that are from a naturally shifted distribution, such as sketches, paintings, and animations of the object categories observed during training. prior work focuses on reducing this gap by designing engineered augmentations of training data or through unsupervised pretraining of a single large model on massive in-the-wild training datasets scraped from the internet. however, the notion of a dataset is also undergoing a paradigm shift in recent years. with drastic improvements in the quality, ease-of-use, and access to modern generative models, generated data is pervading the web. in this light, we study the question: how do these generated datasets influence the natural robustness of image classifiers? we find that imagenet classifiers trained on real data augmented with generated data achieve higher accuracy and effective robustness than standard training and popular augmentation strategies in the presence of natural distribution shifts. we analyze various factors influencing these results, including the choice of conditioning strategies and the amount of generated data. additionally, we find that the standard imagenet classifiers suffer a performance degradation of upto 20\% on the generated data, indicating their fragility at accurately classifying the objects under novel variations. lastly, we demonstrate that the image classifiers, which have been trained on real data augmented with generated data from the base generative model, exhibit greater resilience to natural distribution shifts compared to the classifiers trained on real data augmented with generated data from the finetuned generative model on the real data. the code, models, and datasets are available at this https url.",2023-02-05
"debiasing vision-language models via biased prompts","ching-yao chuang, varun jampani, yuanzhen li, antonio torralba, stefanie jegelka","machine learning","machine learning models have been shown to inherit biases from their training datasets. this can be particularly problematic for vision-language foundation models trained on uncurated datasets scraped from the internet. the biases can be amplified and propagated to downstream applications like zero-shot classifiers and text-to-image generative models. in this study, we propose a general approach for debiasing vision-language foundation models by projecting out biased directions in the text embedding. in particular, we show that debiasing only the text embedding with a calibrated projection matrix suffices to yield robust classifiers and fair generative models. the proposed closed-form solution enables easy integration into large-scale pipelines, and empirical results demonstrate that our approach effectively reduces social bias and spurious correlation in both discriminative and generative vision-language models without the need for additional data or training.",2023-01-31
"sentiment analysis for measuring hope and fear from reddit posts during the 2022 russo-ukrainian conflict","alessio guerra, oktay karakuş","computation and language","this paper proposes a novel lexicon-based unsupervised sentimental analysis method to measure the $``\textit{hope}""$ and $``\textit{fear}""$ for the 2022 ukrainian-russian conflict. $\textit{this http url}$ is utilised as the main source of human reactions to daily events during nearly the first three months of the conflict. the top 50 $``hot""$ posts of six different subreddits about ukraine and news (ukraine, worldnews, ukraina, ukrainianconflict, ukrainewarvideoreport, ukrainewarreports) and their relative comments are scraped and a data set is created. on this corpus, multiple analyses such as (1) public interest, (2) hope/fear score, (3) stock price interaction are employed. we promote using a dictionary approach, which scores the hopefulness of every submitted user post. the latent dirichlet allocation (lda) algorithm of topic modelling is also utilised to understand the main issues raised by users and what are the key talking points. experimental analysis shows that the hope strongly decreases after the symbolic and strategic losses of azovstal (mariupol) and severodonetsk. spikes in hope/fear, both positives and negatives, are present after important battles, but also some non-military events, such as eurovision and football games.",2023-01-19
"leveraging rights of data subjects for social media analysis: studying tiktok via data donations","savvas zannettou, olivia-nemes nemeth, oshrat ayalon, angelica goetzen, krishna p. gummadi, elissa m. redmiles, franziska roesner","social and information networks","tiktok is a relatively novel and widely popular media platform. in response to its expanding user base and cultural impact, researchers are turning to study the platform; however, tiktok, like many social media platforms, restricts external access to data. prior works have acquired data from scraping the platform, user self-reports, and from accounts created by researchers for the study's purpose. existing techniques, while yielding important insights, contain limitations for gathering large-scale quantitative insights on how real tiktok users behave on the platform. we bridge this research gap by implementing a data donation system to collect tiktok data. our system leverages users' right to access their data enabled by the eu's gdpr regulation. we recruit 347 tiktok users, ask them to request their data from tiktok, and then use our system to customize, anonymize, and donate their data. we collect 4.9m videos viewed 9.2m times by our participants -- and associated engagement metrics -- to analyze how people consume content on tiktok, how prevalent liking behavior is on tiktok, and whether there are substantial differences across our participants' demographics. we conclude our work by discussing the lessons learned and future avenues for implementing data donation systems, which we believe offer a promising avenue for collecting user behavioral traces to understand social phenomena through the lens of the web.",2023-01-12
"measuring corporate digital divide with web scraping: evidence from italy","mazzoni leonardo, pinelli fabio, riccaboni massimo","general economics","with the increasing pervasiveness of icts in the fabric of economic activities, the corporate digital divide has emerged as a new crucial topic to evaluate the it competencies and the digital gap between firms and territories. given the scarcity of available granular data to measure the phenomenon, most studies have used survey data. to bridge the empirical gap, we scrape the website homepage of 182 705 italian firms, extracting ten features related to their digital footprint characteristics to develop a new corporate digital assessment index. our results highlight a significant digital divide across dimensions, sectors and geographical locations of italian firms, opening up new perspectives on monitoring and near-real-time data-driven analysis.",2023-01-12
"does progress on imagenet transfer to real-world datasets?","alex fang, simon kornblith, ludwig schmidt","computer vision and pattern recognition","does progress on imagenet transfer to real-world datasets? we investigate this question by evaluating imagenet pre-trained models with varying accuracy (57% - 83%) on six practical image classification datasets. in particular, we study datasets collected with the goal of solving real-world tasks (e.g., classifying images from camera traps or satellites), as opposed to web-scraped benchmarks collected for comparing models. on multiple datasets, models with higher imagenet accuracy do not consistently yield performance improvements. for certain tasks, interventions such as data augmentation improve performance even when architectures do not. we hope that future benchmarks will include more diverse datasets to encourage a more comprehensive approach to improving learning algorithms.",2023-01-11
"learning from what is already out there: few-shot sign language recognition with online dictionaries","matyáš boháček, marek hrúz","computer vision and pattern recognition","today's sign language recognition models require large training corpora of laboratory-like videos, whose collection involves an extensive workforce and financial resources. as a result, only a handful of such systems are publicly available, not to mention their limited localization capabilities for less-populated sign languages. utilizing online text-to-video dictionaries, which inherently hold annotated data of various attributes and sign languages, and training models in a few-shot fashion hence poses a promising path for the democratization of this technology. in this work, we collect and open-source the uwb-sl-wild few-shot dataset, the first of its kind training resource consisting of dictionary-scraped videos. this dataset represents the actual distribution and characteristics of available online sign language data. we select glosses that directly overlap with the already existing datasets wlasl100 and asllvd and share their class mappings to allow for transfer learning experiments. apart from providing baseline results on a pose-based architecture, we introduce a novel approach to training sign language recognition models in a few-shot scenario, resulting in state-of-the-art results on asllvd-skeleton and asllvd-skeleton-20 datasets with top-1 accuracy of $30.97~\%$ and $95.45~\%$, respectively.",2023-01-10
"geode: a geographically diverse evaluation dataset for object recognition","vikram v. ramaswamy, sing yu lin, dora zhao, aaron b. adcock, laurens van der maaten, deepti ghadiyaram, olga russakovsky","computer vision and pattern recognition","current dataset collection methods typically scrape large amounts of data from the web. while this technique is extremely scalable, data collected in this way tends to reinforce stereotypical biases, can contain personally identifiable information, and typically originates from europe and north america. in this work, we rethink the dataset collection paradigm and introduce geode, a geographically diverse dataset with 61,940 images from 40 classes and 6 world regions, and no personally identifiable information, collected through crowd-sourcing. we analyse geode to understand differences in images collected in this manner compared to web-scraping. despite the smaller size of this dataset, we demonstrate its use as both an evaluation and training dataset, highlight shortcomings in current models, as well as show improved performances when even small amounts of geode (1000 - 2000 images per region) are added to a training dataset. we release the full dataset and code at this https url",2023-01-05
"revealed: uncovering pro-eating disorder content on twitter using deep learning","jonathan feldman","machine learning","the covid-19 pandemic induced a vast increase in adolescents diagnosed with eating disorders and hospitalized due to eating disorders. this immense growth stemmed partially from the stress of the pandemic but also from increased exposure to content that promotes eating disorders via social media, which, within the last decade, has become plagued by pro-eating disorder content. this study aimed to create a deep learning model capable of determining whether a given social media post promotes eating disorders based solely on image data. tweets from hashtags that have been documented to promote eating disorders along with tweets from unrelated hashtags were collected. after prepossessing, these images were labeled as either pro-eating disorder or not based on which twitter hashtag they were scraped from. several deep-learning models were trained on the scraped dataset and were evaluated based on their accuracy, f1 score, precision, and recall. ultimately, the vision transformer model was determined to be the most accurate, attaining an f1 score of 0.877 and an accuracy of 86.7% on the test set. the model, which was applied to unlabeled twitter image data scraped from ""#selfie"", uncovered seasonal fluctuations in the relative abundance of pro-eating disorder content, which reached its peak in the summertime. these fluctuations correspond not only to the seasons, but also to stressors, such as the covid-19 pandemic. moreover, the twitter image data indicated that the relative amount of pro-eating disorder content has been steadily rising over the last five years and is likely to continue increasing in the future.",2022-12-28
"contrastive language-vision ai models pretrained on web-scraped multimodal data exhibit sexual objectification bias","robert wolfe, yiwei yang, bill howe, aylin caliskan","computers and society","nine language-vision ai models trained on web scrapes with the contrastive language-image pretraining (clip) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person's human characteristics, such as emotions, are disregarded and the person is treated as a body. we replicate three experiments in psychology quantifying sexual objectification and show that the phenomena persist in ai. a first experiment uses standardized images of women from the sexual objectification and emotion database, and finds that human characteristics are disassociated from images of objectified women: the model's recognition of emotional state is mediated by whether the subject is fully or partially clothed. embedding association tests (eats) return significant effect sizes for both anger (d >0.80) and sadness (d >0.50), associating images of fully clothed subjects with emotions. grad-cam saliency maps highlight that clip gets distracted from emotional expressions in objectified images. a second experiment measures the effect in a representative application: an automatic image captioner (antarctic captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. a third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. a fourth experiment shows that a prompt of ""a [age] year old girl"" generates sexualized images (as determined by an nsfw classifier) up to 73% of the time for vqgan-clip and stable diffusion; the corresponding rate for boys never surpasses 9%. the evidence indicates that language-vision ai models trained on web scrapes learn biases of sexual objectification, which propagate to downstream applications.",2022-12-21
"saved you a click: automatically answering clickbait titles","oliver johnson, beicheng lou, janet zhong, andrey kurenkov","computation and language","often clickbait articles have a title that is phrased as a question or vague teaser that entices the user to click on the link and read the article to find the explanation. we developed a system that will automatically find the answer or explanation of the clickbait hook from the website text so that the user does not need to read through the text themselves. we fine-tune an extractive question and answering model (roberta) and an abstractive one (t5), using data scraped from the 'stopclickbait' facebook pages and reddit's 'savedyouaclick' subforum. we find that both extractive and abstractive models improve significantly after finetuning. we find that the extractive model performs slightly better according to rouge scores, while the abstractive one has a slight edge in terms of bertscores.",2022-12-15
"considerations for differentially private learning with large-scale public pretraining","florian tramèr, gautam kamath, nicholas carlini","machine learning","the performance of differentially private machine learning can be boosted significantly by leveraging the transfer learning capabilities of non-private models pretrained on large public datasets. we critically review this approach. we primarily question whether the use of large web-scraped datasets should be viewed as differential-privacy-preserving. we caution that publicizing these models pretrained on web data as ""private"" could lead to harm and erode the public's trust in differential privacy as a meaningful definition of privacy. beyond the privacy considerations of using public data, we further question the utility of this paradigm. we scrutinize whether existing machine learning benchmarks are appropriate for measuring the ability of pretrained models to generalize to sensitive domains, which may be poorly represented in public web data. finally, we notice that pretraining has been especially impactful for the largest available models -- models sufficiently large to prohibit end users running them on their own devices. thus, deploying such models today could be a net loss for privacy, as it would require (private) data to be outsourced to a more compute-powerful third party. we conclude by discussing potential paths forward for the field of private learning, as public pretraining becomes more popular and powerful.",2022-12-13
"large language models struggle to learn long-tail knowledge","nikhil kandpal, haikang deng, adam roberts, eric wallace, colin raffel","computation and language","the internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. however, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. in this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. in particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. we identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., triviaqa), pre-training corpora (e.g., roots), and model sizes (e.g., 176b parameters). moreover, while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive qa performance on questions with little support in the pre-training data. finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail.",2022-11-15
"safe latent diffusion: mitigating inappropriate degeneration in diffusion models","patrick schramowski, manuel brack, björn deiseroth, kristian kersting","computer vision and pattern recognition","text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. in turn, they may even reinforce such biases. to help combat these undesired side effects, we present safe latent diffusion (sld). specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (i2p)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. as our exhaustive empirical evaluation demonstrates, the introduced sld removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse effect on overall image quality or text alignment.",2022-11-09
"expanding accurate person recognition to new altitudes and ranges: the briar dataset","david cornett iii, joel brogan, nell barber, deniz aykac, seth baird, nick burchfield, carl dukes, andrew duncan, regina ferrell, jim goddard, gavin jager, matt larson, bart murphy, christi johnson, ian shelley, nisha srinivas, brandon stockwell, leanne thompson, matt yohe, robert zhang, scott dolvin, hector j. santos-villalobos, david s. bolme","computer vision and pattern recognition","face recognition technology has advanced significantly in recent years due largely to the availability of large and increasingly complex training datasets for use in deep learning models. these datasets, however, typically comprise images scraped from news sites or social media platforms and, therefore, have limited utility in more advanced security, forensics, and military applications. these applications require lower resolution, longer ranges, and elevated viewpoints. to meet these critical needs, we collected and curated the first and second subsets of a large multi-modal biometric dataset designed for use in the research and development (r&d) of biometric recognition technologies under extremely challenging conditions. thus far, the dataset includes more than 350,000 still images and over 1,300 hours of video footage of approximately 1,000 subjects. to collect this data, we used nikon dslr cameras, a variety of commercial surveillance cameras, specialized long-rage r&d cameras, and group 1 and group 2 uav platforms. the goal is to support the development of algorithms capable of accurately recognizing people at ranges up to 1,000 m and from high angles of elevation. these advances will include improvements to the state of the art in face recognition and will support new research in the area of whole-body recognition using methods based on gait and anthropometry. this paper describes methods used to collect and curate the dataset, and the dataset's characteristics at the current stage.",2022-11-03
"testing independence of exchangeable random variables","marcus hutter","statistics theory","given well-shuffled data, can we determine whether the data items are statistically (in)dependent? formally, we consider the problem of testing whether a set of exchangeable random variables are independent. we will show that this is possible and develop tests that can confidently reject the null hypothesis that data is independent and identically distributed and have high power for (some) exchangeable distributions. we will make no structural assumptions on the underlying sample space. one potential application is in deep learning, where data is often scraped from the whole internet, with duplications abound, which can render data non-iid and test-set evaluation prone to give wrong answers.",2022-10-22
"scrape, cut, paste and learn: automated dataset generation applied to parcel logistics","alexander naumann, felix hertlein, benchun zhou, laura dörr, kai furmans","computer vision and pattern recognition","state-of-the-art approaches in computer vision heavily rely on sufficiently large training datasets. for real-world applications, obtaining such a dataset is usually a tedious task. in this paper, we present a fully automated pipeline to generate a synthetic dataset for instance segmentation in four steps. in contrast to existing work, our pipeline covers every step from data acquisition to the final dataset. we first scrape images for the objects of interest from popular image search engines and since we rely only on text-based queries the resulting data comprises a wide variety of images. hence, image selection is necessary as a second step. this approach of image scraping and selection relaxes the need for a real-world domain-specific dataset that must be either publicly available or created for this purpose. we employ an object-agnostic background removal model and compare three different methods for image selection: object-agnostic pre-processing, manual image selection and cnn-based image selection. in the third step, we generate random arrangements of the object of interest and distractors on arbitrary backgrounds. finally, the composition of the images is done by pasting the objects using four different blending methods. we present a case study for our dataset generation approach by considering parcel segmentation. for the evaluation we created a dataset of parcel photos that were annotated automatically. we find that (1) our dataset generation pipeline allows a successful transfer to real test images (mask ap 86.2), (2) a very accurate image selection process - in contrast to human intuition - is not crucial and a broader category definition can help to bridge the domain gap, (3) the usage of blending methods is beneficial compared to simple copy-and-paste. we made our full code for scraping, image composition and training publicly available at this https url.",2022-10-18
"shadfa 0.1: the iranian movie knowledge graph and graph-embedding-based recommender system","rayhane pouyan, hadi kalamati, hannane ebrahimian, mohammad karrabi, mohammad-r. akbarzadeh-t","information retrieval","movies are a great source of entertainment. however, the problem arises when one is trying to find the desired content within this vast amount of data which is significantly increasing every year. recommender systems can provide appropriate algorithms to solve this problem. the content_based technique has found popularity due to the lack of available user data in most cases. content_based recommender systems are based on the similarity of items' demographic information; term frequency _ inverse document frequency (tf_idf) and knowledge graph embedding (kge) are two approaches used to vectorize data to calculate these similarities. in this paper, we propose a weighted content_based movie rs by combining tf_idf which is an appropriate approach for embedding textual data such as plot/description, and kge which is used to embed named entities such as the director's name. the weights between features are determined using a genetic algorithm. additionally, the iranian movies dataset is created by scraping data from movie_related websites. this dataset and the structure of the farsbase kg are used to create the moviefarsbase kg which is a component in the implementation process of the proposed content_based rs. using precision, recall, and f1 score metrics, this study shows that the proposed approach outperforms the conventional approach that uses tf_idf for embedding all attributes.",2022-10-14
"caption supervision enables robust learners","benjamin feuer, ameya joshi, chinmay hegde","computer vision and pattern recognition","vision language (vl) models like clip are robust to natural distribution shifts, in part because clip learns on unstructured data using a technique called caption supervision; the model inteprets image-linked texts as ground-truth labels. in a carefully controlled comparison study, we show that caption-supervised cnns trained on a standard cross-entropy loss (with image labels assigned by scanning captions for class names) can exhibit greater distributional robustness than vl models trained on the same data. to facilitate future experiments with high-accuracy caption-supervised models, we introduce captionnet (this https url), which includes a class-balanced, fully supervised dataset with over 50,000 new human-labeled imagenet-compliant samples which includes web-scraped captions. in a series of experiments on captionnet, we show how the choice of loss function, data filtration and supervision strategy enable robust computer vision. we also provide the codebase necessary to reproduce our experiments at vl hub (this https url).",2022-10-13
"learning the dynamics of compliant tool-environment interaction for visuo-tactile contact servoing","mark van der merwe, dmitry berenson, nima fazeli","robotics","many manipulation tasks require the robot to control the contact between a grasped compliant tool and the environment, e.g. scraping a frying pan with a spatula. however, modeling tool-environment interaction is difficult, especially when the tool is compliant, and the robot cannot be expected to have the full geometry and physical properties (e.g., mass, stiffness, and friction) of all the tools it must use. we propose a framework that learns to predict the effects of a robot's actions on the contact between the tool and the environment given visuo-tactile perception. key to our framework is a novel contact feature representation that consists of a binary contact value, the line of contact, and an end-effector wrench. we propose a method to learn the dynamics of these contact features from real world data that does not require predicting the geometry of the compliant tool. we then propose a controller that uses this dynamics model for visuo-tactile contact servoing and show that it is effective at performing scraping tasks with a spatula, even in scenarios where precise contact needs to be made to avoid obstacles.",2022-10-07
"data feedback loops: model-driven amplification of dataset biases","rohan taori, tatsunori b. hashimoto","machine learning","datasets scraped from the internet have been critical to the successes of large-scale machine learning. yet, this very success puts the utility of future internet-derived datasets at potential risk, as model outputs begin to replace human annotations as a source of supervision. in this work, we first formalize a system where interactions with one model are recorded as history and scraped as training data in the future. we then analyze its stability over time by tracking changes to a test-time bias statistic (e.g. gender bias of model predictions). we find that the degree of bias amplification is closely linked to whether the model's outputs behave like samples from the training distribution, a behavior which we characterize and define as consistent calibration. experiments in three conditional prediction scenarios - image classification, visual role-labeling, and language generation - demonstrate that models that exhibit a sampling-like behavior are more calibrated and thus more stable. based on this insight, we propose an intervention to help calibrate and stabilize unstable feedback systems. code is available at this https url.",2022-09-08
"an assessment tool for academic research managers in the third world","fernando delbianco, andres fioriti, fernando tohmé","econometrics","the academic evaluation of the publication record of researchers is relevant for identifying talented candidates for promotion and funding. a key tool for this is the use of the indexes provided by web of science and scopus, costly databases that sometimes exceed the possibilities of academic institutions in many parts of the world. we show here how the data in one of the bases can be used to infer the main index of the other one. methods of data analysis used in machine learning allow us to select just a few of the hundreds of variables in a database, which later are used in a panel regression, yielding a good approximation to the main index in the other database. since the information of scopus can be freely scraped from the web, this approach allows to infer for free the impact factor of publications, the main index used in research assessments around the globe.",2022-09-07
"a topic-aware graph neural network model for knowledge base updating","jiajun tong, zhixiao wang, xiaobin rui","information retrieval","the open domain knowledge base is very important. it is usually extracted from encyclopedia websites and is widely used in knowledge retrieval systems, question answering systems, or recommendation systems. in practice, the key challenge is to maintain an up-to-date knowledge base. different from unwieldy fetching all of the data from the encyclopedia dumps, to enlarge the freshness of the knowledge base as big as possible while avoiding invalid fetching, the current knowledge base updating methods usually determine whether entities need to be updated by building a prediction model. however, these methods can only be defined in some specific fields and the result turns out to be obvious bias, due to the problem of data source and data structure. the users' query intentions are often diverse as to the open domain knowledge, so we construct a topic-aware graph network for knowledge updating based on the user query log. our methods can be summarized as follow: 1. extract entities through the user's log and select them as seeds 2. scrape the attributes of seed entities in the encyclopedia website, and self-supervised construct the entity attribute graph for each entity. 3. use the entity attribute graph to train the gnn entity update model to determine whether the entity needs to be synchronized. 4.use the encyclopedia knowledge to match and update the filtered entity with the entity in the knowledge base according to the minimum edit times algorithm.",2022-08-31
"data-driven modeling of beam loss in the lhc","ekaterina krymova, guillaume obozinski, michael schenk, loic coyle, tatiana pieloni","accelerator physics","in the large hadron collider, the beam losses are continuously measured for machine protection. by design, most of the particle losses occur in the collimation system, where the particles with high oscillation amplitudes or large momentum error are scraped from the beams. the level of particle losses typically is optimized manually by changing multiple control parameters, among which are, for example, currents in the focusing and defocusing magnets along the collider. it is generally challenging to model and predict losses based on the control parameters due to various (non-linear) effects in the system, such as electron clouds, resonance effects, etc, and multiple sources of uncertainty. at the same time understanding the influence of control parameters on the losses is extremely important in order to improve the operation and performance, and future design of accelerators. existing results showed that it is hard to generalize the models, which assume the regression model of losses depending on control parameters, from fills carried out throughout one year to the data of another year. to circumvent this, we propose to use an autoregressive modeling approach, where we take into account not only the observed control parameters but also previous loss values. we use an equivalent kalman filter (kf) formulation in order to efficiently estimate models with different lags.",2022-08-18
"image-based detection of surface defects in concrete during construction","dominik kuhnke, monika kwiatkowski, olaf hellwich","computer vision and pattern recognition","defects increase the cost and duration of construction projects as they require significant inspection and documentation efforts. automating defect detection could significantly reduce these efforts. this work focuses on detecting honeycombs, a substantial defect in concrete structures that may affect structural integrity. we compared honeycomb images scraped from the web with images obtained from real construction inspections. we found that web images do not capture the complete variance found in real-case scenarios and that there is still a lack of data in this domain. our dataset is therefore freely available for further research. a mask r-cnn and efficientnet-b0 were trained for honeycomb detection. the mask r-cnn model allows detecting honeycombs based on instance segmentation, whereas the efficientnet-b0 model allows a patch-based classification. our experiments demonstrate that both approaches are suitable for solving and automating honeycomb detection. in the future, this solution can be incorporated into defect documentation systems.",2022-08-03
"openfilter: a framework to democratize research access to social media ar filters","piera riccio, bill psomas, francesco galati, francisco escolano, thomas hofmann, nuria oliver","computer vision and pattern recognition","augmented reality or ar filters on selfies have become very popular on social media platforms for a variety of applications, including marketing, entertainment and aesthetics. given the wide adoption of ar face filters and the importance of faces in our social structures and relations, there is increased interest by the scientific community to analyze the impact of such filters from a psychological, artistic and sociological perspective. however, there are few quantitative analyses in this area mainly due to a lack of publicly available datasets of facial images with applied ar filters. the proprietary, close nature of most social media platforms does not allow users, scientists and practitioners to access the code and the details of the available ar face filters. scraping faces from these platforms to collect data is ethically unacceptable and should, therefore, be avoided in research. in this paper, we present openfilter, a flexible framework to apply ar filters available in social media platforms on existing large collections of human faces. moreover, we share fairbeauty and b-lfw, two beautified versions of the publicly available fairface and lfw datasets and we outline insights derived from the analysis of these beautified datasets.",2022-07-19
"visualizing non-fungible token ethics: a case study on cryptopunks","yufan zhang, zichao chen, luyao zhang, xin tong","human-computer interaction","as a blockchain-based application, non-fungible token (nft) has received worldwide attention over the past few years. digital artwork is the main form of nft that can be stored on different blockchains. although the nft market is rapidly developing, we observed potential ethical and racial fairness issues in the design of nft artworks due to a lack of ethical guidelines or censorship. therefore, we investigated cryptopunks, the most famous collection in the nft market, to explore and visualize its potential ethical issues. we explored the ethical issues from three aspects: design, trading transactions, and related topics on twitter. we scraped data from twitter and dune analytics using python libraries, twitter crawler, and sentiment analysis tools. our five visualizations implied that 1.6 times more male punks were created in the initial design process than the female ones. and the male ones have a higher average selling price than females; lighter-skinned punks tend to sell for higher prices. the results of our study and visualizations provide a preliminary exploration of cryptopunks and further inspire future ethical-related investigation and research in the nft domain.",2022-06-26
"data-driven model for divertor plasma detachment prediction","ben zhu, menglong zhao, harsh bhatia, xue-qiao xu, peer-timo bremer, william meyer, nami li, thomas rognlien","plasma physics","we present a fast and accurate data-driven surrogate model for divertor plasma detachment prediction leveraging the latent feature space concept in machine learning research. our approach involves constructing and training two neural networks. an autoencoder that finds a proper latent space representation (lsr) of plasma state by compressing the multi-modal diagnostic measurements, and a forward model using multi-layer perception (mlp) that projects a set of plasma control parameters to its corresponding lsr. by combining the forward model and the decoder network from autoencoder, this new data-driven surrogate model is able to predict a consistent set of diagnostic measurements based on a few plasma control parameters. in order to ensure that the crucial detachment physics is correctly captured, highly efficient 1d uedge model is used to generate training and validation data in this study. benchmark between the data-driven surrogate model and uedge simulations shows that our surrogate model is capable to provide accurate detachment prediction (usually within a few percent relative error margin) but with at least four orders of magnitude speed-up, indicating that performance-wise, it has the potential to facilitate integrated tokamak design and plasma control. comparing to the widely used two-point model and/or two-point model formatting, the new data-driven model features additional detachment front prediction and can be easily extended to incorporate richer physics. this study demonstrates that the complicated divertor and scrape-off-layer plasma state has a low-dimensional representation in latent space. understanding plasma dynamics in latent space and utilizing this knowledge could open a new path for plasma control in magnetic fusion energy research.",2022-06-20
"zero-shot video question answering via frozen bidirectional language models","antoine yang, antoine miech, josef sivic, ivan laptev, cordelia schmid","computer vision and pattern recognition","video question answering (videoqa) is a complex task that requires diverse multi-modal data for training. manual annotation of question and answers for videos, however, is tedious and prohibits scalability. to tackle this problem, recent methods consider zero-shot settings with no manual annotation of visual question-answer. in particular, a promising approach adapts frozen autoregressive language models pretrained on web-scale text-only data to multi-modal inputs. in contrast, we here build on frozen bidirectional language models (bilm) and show that such an approach provides a stronger and cheaper alternative for zero-shot videoqa. in particular, (i) we combine visual inputs with the frozen bilm using light trainable modules, (ii) we train such modules using web-scraped multi-modal data, and finally (iii) we perform zero-shot videoqa inference through masked language modeling, where the masked text is the answer to a given question. our proposed approach, frozenbilm, outperforms the state of the art in zero-shot videoqa by a significant margin on a variety of datasets, including lsmdc-fib, ivqa, msrvtt-qa, msvd-qa, activitynet-qa, tgif-frameqa, how2qa and tvqa. it also demonstrates competitive performance in the few-shot and fully-supervised setting. our code and models are publicly available at this https url.",2022-06-16
"improving generalization by mimicking the human visual diet","spandan madan, you li, mengmi zhang, hanspeter pfister, gabriel kreiman","computer vision and pattern recognition","we present a new perspective on bridging the generalization gap between biological and computer vision -- mimicking the human visual diet. while computer vision models rely on internet-scraped datasets, humans learn from limited 3d scenes under diverse real-world transformations with objects in natural context. our results demonstrate that incorporating variations and contextual cues ubiquitous in the human visual training data (visual diet) significantly improves generalization to real-world transformations such as lighting, viewpoint, and material changes. this improvement also extends to generalizing from synthetic to real-world data -- all models trained with a human-like visual diet outperform specialized architectures by large margins when tested on natural image data. these experiments are enabled by our two key contributions: a novel dataset capturing scene context and diverse real-world transformations to mimic the human visual diet, and a transformer model tailored to leverage these aspects of the human visual diet. all data and source code can be accessed at this https url.",2022-06-15
"prioritized training on points that are learnable, worth learning, and not yet learnt","sören mindermann, jan brauner, muhammed razzak, mrinank sharma, andreas kirsch, winnie xu, benedikt höltgen, aidan n. gomez, adrien morisot, sebastian farquhar, yarin gal","machine learning","training on web-scale data can take months. but most computation and time is wasted on redundant and noisy points that are already learnt or not learnable. to accelerate training, we introduce reducible holdout loss selection (rho-loss), a simple but principled technique which selects approximately those points for training that most reduce the model's generalization loss. as a result, rho-loss mitigates the weaknesses of existing data selection methods: techniques from the optimization literature typically select 'hard' (e.g. high loss) points, but such points are often noisy (not learnable) or less task-relevant. conversely, curriculum learning prioritizes 'easy' points, but such points need not be trained on once learned. in contrast, rho-loss selects points that are learnable, worth learning, and not yet learnt. rho-loss trains in far fewer steps than prior art, improves accuracy, and speeds up training on a wide range of datasets, hyperparameters, and architectures (mlps, cnns, and bert). on the large web-scraped image dataset clothing-1m, rho-loss trains in 18x fewer steps and reaches 2% higher final accuracy than uniform data shuffling.",2022-06-14
"autoregressive perturbations for data poisoning","pedro sandoval-segura, vasu singla, jonas geiping, micah goldblum, tom goldstein, david w. jacobs","machine learning","the prevalence of data scraping from social media as a means to obtain datasets has led to growing concerns regarding unauthorized use of data. data poisoning attacks have been proposed as a bulwark against scraping, as they make data ""unlearnable"" by adding small, imperceptible perturbations. unfortunately, existing methods require knowledge of both the target architecture and the complete dataset so that a surrogate network can be trained, the parameters of which are used to generate the attack. in this work, we introduce autoregressive (ar) poisoning, a method that can generate poisoned data without access to the broader dataset. the proposed ar perturbations are generic, can be applied across different datasets, and can poison different architectures. compared to existing unlearnable methods, our ar poisons are more resistant against common defenses such as adversarial training and strong data augmentations. our analysis further provides insight into what makes an effective data poison.",2022-06-08
"squality: building a long-document summarization dataset the hard way","alex wang, richard yuanzhe pang, angelica chen, jason phang, samuel r. bowman","computation and language","summarization datasets are often assembled either by scraping naturally occurring public-domain summaries -- which are nearly always in difficult-to-work-with technical domains -- or by using approximate heuristics to extract them from everyday text -- which frequently yields unfaithful summaries. in this work, we turn to a slower but more straightforward approach to developing summarization benchmark data: we hire highly-qualified contractors to read stories and write original summaries from scratch. to amortize reading time, we collect five summaries per document, with the first giving an overview and the subsequent four addressing specific questions. we use this protocol to collect squality, a dataset of question-focused summaries built on the same public-domain short stories as the multiple-choice dataset quality (pang et al., 2021). experiments with state-of-the-art summarization systems show that our dataset is challenging and that existing automatic evaluation metrics are weak indicators of quality.",2022-05-23
"greendb: toward a product-by-product sustainability database","sebastian jäger, jessica greene, max jakob, ruben korenke, tilman santarius, felix biessmann","machine learning","the production, shipping, usage, and disposal of consumer goods have a substantial impact on greenhouse gas emissions and the depletion of resources. modern retail platforms rely heavily on machine learning (ml) for their search and recommender systems. thus, ml can potentially support efforts towards more sustainable consumption patterns, for example, by accounting for sustainability aspects in product search or recommendations. however, leveraging ml potential for reaching sustainability goals requires data on sustainability. unfortunately, no open and publicly available database integrates sustainability information on a product-by-product basis. in this work, we present the greendb, which fills this gap. based on search logs of millions of users, we prioritize which products users care about most. the greendb schema extends the well-known this http url product definition and can be readily integrated into existing product catalogs to improve sustainability information available for search and recommendation experiences. we present our proof of concept implementation of a scraping system that creates the greendb dataset.",2022-05-05
"speaker recognition in the wild","neeraj chhimwal, anirudh gupta, rishabh gaur, harveen singh chadha, priyanshi shah, ankur dhuriya, vivek raghavan","sound","in this paper, we propose a pipeline to find the number of speakers, as well as audios belonging to each of these now identified speakers in a source of audio data where number of speakers or speaker labels are not known a priori. we used this approach as a part of our data preparation pipeline for speech recognition in indic languages (this https url). to understand and evaluate the accuracy of our proposed pipeline, we introduce two metrics: cluster purity, and cluster uniqueness. cluster purity quantifies how ""pure"" a cluster is. cluster uniqueness, on the other hand, quantifies what percentage of clusters belong only to a single dominant speaker. we discuss more on these metrics in section \ref{sec:metrics}. since we develop this utility to aid us in identifying data based on speaker ids before training an automatic speech recognition (asr) model, and since most of this data takes considerable effort to scrape, we also conclude that 98\% of data gets mapped to the top 80\% of clusters (computed by removing any clusters with less than a fixed number of utterances -- we do this to get rid of some very small clusters and use this threshold as 30), in the test set chosen.",2022-05-05
"making large language models interactive: a pioneer study on supporting complex information-seeking tasks with implicit constraints","ali ahmadvand, negar arabzadeh, julia kiseleva, patricio figueroa sanz, xin deng, sujay jauhar, michael gamon, eugene agichtein, ned friend, aniruddha","information retrieval","current interactive systems with natural language interfaces lack the ability to understand a complex information-seeking request which expresses several implicit constraints at once, and there is no prior information about user preferences e.g.,""find hiking trails around san francisco which are accessible with toddlers and have beautiful scenery in summer"", where output is a list of possible suggestions for users to start their exploration. in such scenarios, user requests can be issued in one shot in the form of a complex and long query, unlike conversational and exploratory search models, where require short utterances or queries are often presented to the system step by step. we have designed and deployed a platform to collect the data from approaching such complex interactive systems. moreover, despite with the current advancement of generative language models these models suffer from hallucination in providing accurate factual knowledge. all language models are mostly trained in large part on web-scraped data from the past, which usually is not useful for immediate users' needs. in this article, we propose an ia that leverages large language models (llm) for complex request understanding and makes it interactive using reinforcement learning that allows intricately refine user requests by making them complete, leading to better retrieval and reduce llms hallucination problems for current user needs. to demonstrate the performance of the proposed modeling paradigm, we have adopted various pre-retrieval metrics that capture the extent to which guided interactions with our system yield better retrieval results. through extensive experimentation, we demonstrated that our method significantly outperforms several robust baselines.",2022-05-02
"l3cube-hingcorpus and hingbert: a code mixed hindi-english dataset and bert language models","ravindra nayak, raviraj joshi","computation and language","code-switching occurs when more than one language is mixed in a given sentence or a conversation. this phenomenon is more prominent on social media platforms and its adoption is increasing over time. therefore code-mixed nlp has been extensively studied in the literature. as pre-trained transformer-based architectures are gaining popularity, we observe that real code-mixing data are scarce to pre-train large language models. we present l3cube-hingcorpus, the first large-scale real hindi-english code mixed data in a roman script. it consists of 52.93m sentences and 1.04b tokens, scraped from twitter. we further present hingbert, hingmbert, hingroberta, and hinggpt. the bert models have been pre-trained on codemixed hingcorpus using masked language modelling objectives. we show the effectiveness of these bert models on the subsequent downstream tasks like code-mixed sentiment analysis, pos tagging, ner, and lid from the gluecos benchmark. the hinggpt is a gpt2 based generative transformer model capable of generating full tweets. we also release l3cube-hinglid corpus, the largest code-mixed hindi-english language identification(lid) dataset and hingbert-lid, a production-quality lid model to facilitate capturing of more code-mixed data using the process outlined in this work. the dataset and models are available at this https url .",2022-04-18
"product market demand analysis using nlp in banglish text with sentiment analysis and named entity recognition","md sabbir hossain, nishat nayla, annajiat alim rasel","computation and language","product market demand analysis plays a significant role for originating business strategies due to its noticeable impact on the competitive business field. furthermore, there are roughly 228 million native bengali speakers, the majority of whom use banglish text to interact with one another on social media. consumers are buying and evaluating items on social media with banglish text as social media emerges as an online marketplace for entrepreneurs. people use social media to find preferred smartphone brands and models by sharing their positive and bad experiences with them. for this reason, our goal is to gather banglish text data and use sentiment analysis and named entity identification to assess bangladeshi market demand for smartphones in order to determine the most popular smartphones by gender. we scraped product related data from social media with instant data scrapers and crawled data from wikipedia and other sites for product information with python web scrapers. using python's pandas and seaborn libraries, the raw data is filtered using nlp methods. to train our datasets for named entity recognition, we utilized spacey's custom ner model, amazon comprehend custom ner. a tensorflow sequential model was deployed with parameter tweaking for sentiment analysis. meanwhile, we used the google cloud translation api to estimate the gender of the reviewers using the banglalinga library. in this article, we use natural language processing (nlp) approaches and several machine learning models to identify the most in-demand items and services in the bangladeshi market. our model has an accuracy of 87.99% in spacy custom named entity recognition, 95.51% in amazon comprehend custom ner, and 87.02% in the sequential model for demand analysis. after spacy's study, we were able to manage 80% of mistakes related to misspelled words using a mix of levenshtein distance and ratio algorithms.",2022-04-04
"the ocean mailing list data set: network analysis spanning mailing lists and code repositories","melanie warrick, samuel f. rosenblatt, jean-gabriel young, amanda casari, laurent hébert-dufresne, james bagrow","computers and society","communication surrounding the development of an open source project largely occurs outside the software repository itself. historically, large communities often used a collection of mailing lists to discuss the different aspects of their projects. multimodal tool use, with software development and communication happening on different channels, complicates the study of open source projects as a sociotechnical system. here, we combine and standardize mailing lists of the python community, resulting in 954,287 messages from 1995 to the present. we share all scraping and cleaning code to facilitate reproduction of this work, as well as smaller datasets for the golang (122,721 messages), angular (20,041 messages) and node.js (12,514 messages) communities. to showcase the usefulness of these data, we focus on the cpython repository and merge the technical layer (which github account works on what file and with whom) with the social layer (messages from unique email addresses) by identifying 33% of github contributors in the mailing list data. we then explore correlations between the valence of social messaging and the structure of the collaboration network. we discuss how these data provide a laboratory to test theories from standard organizational science in large open source projects.",2022-04-01
"does corpus quality really matter for low-resource languages?","mikel artetxe, itziar aldabe, rodrigo agerri, olatz perez-de-viñaspre, aitor soroa","computation and language","the vast majority of non-english corpora are derived from automatically filtered versions of commoncrawl. while prior work has identified major issues on the quality of these datasets (kreutzer et al., 2021), it is not clear how this impacts downstream performance. taking representation learning in basque as a case study, we explore tailored crawling (manually identifying and scraping websites with high-quality content) as an alternative to filtering commoncrawl. our new corpus, called euscrawl, is similar in size to the basque portion of popular multilingual corpora like cc100 and mc4, yet it has a much higher quality according to native annotators. for instance, 66% of documents are rated as high-quality for euscrawl, in contrast with <33% for both mc4 and cc100. nevertheless, we obtain similar results on downstream nlu tasks regardless of the corpus used for pre-training. our work suggests that nlu performance in low-resource languages is not primarily constrained by the quality of the data, and other factors like corpus size and domain coverage can play a more important role.",2022-03-15
"do language models plagiarize?","jooyoung lee, thai le, jinghui chen, dongwon lee","computation and language","past literature has illustrated that language models (lms) often memorize parts of training instances and reproduce them in natural language generation (nlg) processes. however, it is unclear to what extent lms ""reuse"" a training corpus. for instance, models can generate paraphrased sentences that are contextually similar to training samples. in this work, therefore, we study three types of plagiarism (i.e., verbatim, paraphrase, and idea) among gpt-2 generated texts, in comparison to its training data, and further analyze the plagiarism patterns of fine-tuned lms with domain-specific corpora which are extensively used in practice. our results suggest that (1) three types of plagiarism widely exist in lms beyond memorization, (2) both size and decoding methods of lms are strongly associated with the degrees of plagiarism they exhibit, and (3) fine-tuned lms' plagiarism patterns vary based on their corpus similarity and homogeneity. given that a majority of lms' training data is scraped from the web without informing content owners, their reiteration of words, phrases, and even core ideas from training sets into generated texts has ethical implications. their patterns are likely to exacerbate as both the size of lms and their training data increase, raising concerns about indiscriminately pursuing larger models with larger training corpora. plagiarized content can also contain individuals' personal and sensitive information. these findings overall cast doubt on the practicality of current lms in mission-critical writing tasks and urge more discussions around the observed phenomena. data and source code are available at this https url.",2022-03-15
"indicnlg benchmark: multilingual datasets for diverse nlg tasks in indic languages","aman kumar, himani shrotriya, prachi sahu, raj dabre, ratish puduppully, anoop kunchukuttan, amogh mishra, mitesh m. khapra, pratyush kumar","computation and language","natural language generation (nlg) for non-english languages is hampered by the scarcity of datasets in these languages. in this paper, we present the indicnlg benchmark, a collection of datasets for benchmarking nlg for 11 indic languages. we focus on five diverse tasks, namely, biography generation using wikipedia infoboxes, news headline generation, sentence summarization, paraphrase generation and, question generation. we describe the created datasets and use them to benchmark the performance of several monolingual and multilingual baselines that leverage pre-trained sequence-to-sequence models. our results exhibit the strong performance of multilingual language-specific pre-trained models, and the utility of models trained on our dataset for other related nlg tasks. our dataset creation methods can be easily applied to modest-resource languages as they involve simple steps such as scraping news articles and wikipedia infoboxes, light cleaning, and pivoting through machine translation data. to the best of our knowledge, the indicnlg benchmark is the first nlg benchmark for indic languages and the most diverse multilingual nlg dataset, with approximately 8m examples across 5 tasks and 11 languages. the datasets and models are publicly available at this https url.",2022-03-10
"deduplicating training data mitigates privacy risks in language models","nikhil kandpal, eric wallace, colin raffel","cryptography and security","past work has shown that large language models are susceptible to privacy attacks, where adversaries generate sequences from a trained model and detect which sequences are memorized from the training set. in this work, we show that the success of these attacks is largely due to duplication in commonly used web-scraped training sets. we first show that the rate at which language models regenerate training sequences is superlinearly related to a sequence's count in the training set. for instance, a sequence that is present 10 times in the training data is on average generated ~1000 times more often than a sequence that is present only once. we next show that existing methods for detecting memorized sequences have near-chance accuracy on non-duplicated training sequences. finally, we find that after applying methods to deduplicate training data, language models are considerably more secure against these types of privacy attacks. taken together, our results motivate an increased focus on deduplication in privacy-sensitive applications and a reevaluation of the practicality of existing privacy attacks.",2022-02-14
"data analysis of bulacan state university faculty scientific publication based on google scholar using web data scraping technique","jayson m. victoriano, jaime p. pulumbarit, luisito lolong lacatan, richard albert s. salivio, rica louise a. barawid","digital libraries","the paper aims to analyze and monitor the research publication productivity of the faculty members of bulacan state university. this paper compiles all the scientific publications from bulacan state university (bulsu) and its external campuses as an index in google scholar. this study was intended to track and monitor the scientific productivity of the faculty members of bulacan state university and each college and campus.",2022-02-12
"tess transit timing of hundreds of hot jupiters","ekaterina s. ivshina, joshua n. winn","earth and planetary astrophysics","we provide a database of transit times and updated ephemerides for 382 planets based on data from the nasa transiting exoplanet survey satellite (tess) and previously reported transit times which were scraped from the literature in a semi-automated fashion. in total, our database contains 8,667 transit timing measurements for 382 systems. about 240 planets in the catalog are hot jupiters (i.e. planets with mass $>$0.3$m_{\rm jup}$ and period $<$10 days) that have been observed by tess. the new ephemerides are useful for scheduling follow-up observations and searching for long-term period changes. wasp-12 remains the only system for which a period change is securely detected. we remark on other cases of interest, such as a few systems with suggestive (but not yet convincing) evidence for period changes, and the detection of a second transiting planet in the ngts-11 system. the compilation of light curves, transit times, ephemerides, and timing residuals are made available online, along with the python code that generated them (visit this https url).",2022-02-07
"l3cube-mahacorpus and mahabert: marathi monolingual corpus, marathi bert language models, and resources","raviraj joshi","computation and language","we present l3cube-mahacorpus a marathi monolingual data set scraped from different internet sources. we expand the existing marathi monolingual corpus with 24.8m sentences and 289m tokens. we further present, mahabert, mahaalbert, and maharoberta all bert-based masked language models, and mahaft, the fast text word embeddings both trained on full marathi corpus with 752m tokens. we show the effectiveness of these resources on downstream marathi sentiment analysis, text classification, and named entity recognition (ner) tasks. we also release mahagpt, a generative marathi gpt model trained on marathi corpus. marathi is a popular language in india but still lacks these resources. this work is a step forward in building open resources for the marathi language. the data and models are available at this https url .",2022-02-02
"lagoon: an analysis tool for open source communities","sourya dey, walt woods","social and information networks","this paper presents lagoon -- an open source platform for understanding the complex ecosystems of open source software (oss) communities. the platform currently utilizes spatiotemporal graphs to store and investigate the artifacts produced by these communities, and help analysts identify bad actors who might compromise an oss project's security. lagoon provides ingest of artifacts from several common sources, including source code repositories, issue trackers, mailing lists and scraping content from project websites. ingestion utilizes a modular architecture, which supports incremental updates from data sources and provides a generic identity fusion process that can recognize the same community members across disparate accounts. a user interface is provided for visualization and exploration of an oss project's complete sociotechnical graph. scripts are provided for applying machine learning to identify patterns within the data. while current focus is on the identification of bad actors in the python community, the platform's reusability makes it easily extensible with new data and analyses, paving the way for lagoon to become a comprehensive means of assessing various oss-based projects and their communities.",2022-01-26
"datasheet for the pile","stella biderman, kieran bicheno, leo gao","computation and language","this datasheet describes the pile, a 825 gib dataset of human-authored text compiled by eleutherai for use in large-scale language modeling. the pile is comprised of 22 different text sources, ranging from original scrapes done for this project, to text data made available by the data owners, to third-party scrapes available online.",2022-01-13
"analysis of longitudinal changes in privacy behavior of android applications","alexander yu, yuvraj agarwal, jason i. hong","cryptography and security","privacy concerns have long been expressed around smart devices, and the concerns around android apps have been studied by many past works. over the past 10 years, we have crawled and scraped data for almost 1.9 million apps, and also stored the apks for 135,536 of them. in this paper, we examine the trends in how android apps have changed over time with respect to privacy and look at it from two perspectives: (1) how privacy behavior in apps have changed as they are updated over time, (2) how these changes can be accounted for when comparing third-party libraries and the app's own internals. to study this, we examine the adoption of https, whether apps scan the device for other installed apps, the use of permissions for privacy-sensitive data, and the use of unique identifiers. we find that privacy-related behavior has improved with time as apps continue to receive updates, and that the third-party libraries used by apps are responsible for more issues with privacy. however, we observe that in the current state of android apps, there has not been enough of an improvement in terms of privacy and many issues still need to be addressed.",2021-12-28
"estimating dynamical parameters of two interacting galaxies using deep learning","adarsh mahor, janvita reddy, amitesh singh, shashwat singh","astrophysics of galaxies","the science behind galaxy interaction and mergers has a fundamental role and gives us an insight into galaxy formation and its evolution. fluctuating angular momentum is responsible for extraordinary events like polar rings, tidal tails, and ripples. to study different phenomena related to galaxy interactions, various parameters like the mass ratio of the interacting galaxy, orbital parameters, mass distribution, morphologies are required. convolutional neural networks (cnn) are widely used to classify image data. thus, we used cnn as our approach to the problem. in this work, we will be using data from state-of-the-art magneto-hydrodynamic simulations of galaxy mergers from the galmer database at different dynamical parameters using image snapshots of merging pairs of galaxies and feeding them to our deep learning model (resnet). the dynamical parameters we are aiming for; would be spin, relative inclination ($i$), viewing angle ($\theta$), and azimuthal angle ($\phi$). we aim to download bulk data using the web scraping method. the first approach is to create different combinations of these parameters to form 60 classes. feeding the data into the model, we achieved 93.63% accuracy. as we received good results in minute classification, we moved to our second approach, regression. here the model can predict the continuous and exact values of the dynamical parameters. we have achieved a 99.86% r-squared value and the mean squared error of 0.0833 on testing data. in the end, we used data from sloan digital sky survey to test our trained model on some real images.",2021-12-23
"vizextract: automatic relation extraction from data visualizations","dale decatur, sanjay krishnan","computer vision and pattern recognition","visual graphics, such as plots, charts, and figures, are widely used to communicate statistical conclusions. extracting information directly from such visualizations is a key sub-problem for effective search through scientific corpora, fact-checking, and data extraction. this paper presents a framework for automatically extracting compared variables from statistical charts. due to the diversity and variation of charting styles, libraries, and tools, we leverage a computer vision based framework to automatically identify and localize visualization facets in line graphs, scatter plots, or bar graphs and can include multiple series per graph. the framework is trained on a large synthetically generated corpus of matplotlib charts and we evaluate the trained model on other chart datasets. in controlled experiments, our framework is able to classify, with 87.5% accuracy, the correlation between variables for graphs with 1-3 series per graph, varying colors, and solid line styles. when deployed on real-world graphs scraped from the internet, it achieves 72.8% accuracy (81.2% accuracy when excluding ""hard"" graphs). when deployed on the figureqa dataset, it achieves 84.7% accuracy.",2021-12-07
"gap enhancing semantic interoperability of genomic datasets and provenance through nanopublications","matheus feijoó, rodrigo jardim, sergio serra, maria luiza campos","databases","while the publication of datasets in scientific repositories has become broadly recognised, the repositories tend to have increasing semantic-related problems. for instance, they present various data reuse obstacles for machine-actionable processes, especially in biological repositories, hampering the reproducibility of scientific experiments. an example of these shortcomings is the genbank database. we propose gap, an innovative data model to enhance the semantic data meaning to address these issues. the model focuses on converging related approaches like data provenance, semantic interoperability, fair principles, and nanopublications. our experiments include a prototype to scrape genomic data and trace them to nanopublications as a proof of concept. for this, (meta)data are stored in a three-level nanopub data model. the first level is related to a target organism, specifying data in terms of biological taxonomy. the second level focuses on the biological strains of the target, the central part of our contribution. the strains express information related to deciphered (meta)data of the genetic variations of the genomic material. the third level stores related scientific papers (meta)data. we expect it will offer higher data storage flexibility and more extensive interoperability with other data sources by incorporating and adopting associated approaches to store genomic data in the proposed model.",2021-11-16
"a ground-truth dataset of real security patches","sofia reis, rui abreu","cryptography and security","training machine learning approaches for vulnerability identification and producing reliable tools to assist developers in implementing quality software -- free of vulnerabilities -- is challenging due to the lack of large datasets and real data. researchers have been looking at these issues and building datasets. however, these datasets usually miss natural language artifacts and programming language diversity. we scraped the entire cve details database for github references and augmented the data with 3 security-related datasets. we used the data to create a ground-truth dataset of natural language artifacts (such as commit messages, commits comments, and summaries), meta-data and code changes. our dataset integrates a total of 8057 security-relevant commits -- the equivalent to 5942 security patches -- from 1339 different projects spanning 146 different types of vulnerabilities and 20 languages. a dataset of 110k non-security-related commits is also provided. data and scripts are all available on github. data is stored in a .csv file. codebases can be downloaded using our scripts. our dataset is a valuable asset to answer research questions on different topics such as the identification of security-relevant information using nlp models; software engineering and security best practices; and, vulnerability detection and patching; and, security program analysis.",2021-10-18
"contextual hate speech detection in code mixed text using transformer based approaches","ravindra nayak, raviraj joshi","computation and language","in the recent past, social media platforms have helped people in connecting and communicating to a wider audience. but this has also led to a drastic increase in cyberbullying. it is essential to detect and curb hate speech to keep the sanity of social media platforms. also, code mixed text containing more than one language is frequently used on these platforms. we, therefore, propose automated techniques for hate speech detection in code mixed text from scraped twitter. we specifically focus on code mixed english-hindi text and transformer-based approaches. while regular approaches analyze the text independently, we also make use of content text in the form of parent tweets. we try to evaluate the performances of multilingual bert and indic-bert in single-encoder and dual-encoder settings. the first approach is to concatenate the target text and context text using a separator token and get a single representation from the bert model. the second approach encodes the two texts independently using a dual bert encoder and the corresponding representations are averaged. we show that the dual-encoder approach using independent representations yields better performance. we also employ simple ensemble methods to further improve the performance. using these methods we report the best f1 score of 73.07% on the hasoc 2021 ichcl code mixed data set.",2021-10-18
"presenting a larger up-to-date movie dataset and investigating the effects of pre-released attributes on gross revenue","arnab sen sharma, tirtha roy, sadique ahmmod rifat, maruf ahmed mridul","information retrieval","movie-making has become one of the most costly and risky endeavors in the entertainment industry. continuous change in the preference of the audience makes it harder to predict what kind of movie will be financially successful at the box office. so, it is no wonder that cautious, intelligent stakeholders and large production houses will always want to know the probable revenue that will be generated by a movie before making an investment. researchers have been working on finding an optimal strategy to help investors in making the right decisions. but the lack of a large, up-to-date dataset makes their work harder. in this work, we introduce an up-to-date, richer, and larger dataset that we have prepared by scraping imdb for researchers and data analysts to work with. the compiled dataset contains the summery data of 7.5 million titles and detail information of more than 200k movies. additionally, we perform different statistical analysis approaches on our dataset to find out how a movie's revenue is affected by different pre-released attributes such as budget, runtime, release month, content rating, genre etc. in our analysis, we have found that having a star cast/director has a positive impact on generated revenue. we introduce a novel approach for calculating the star power of a movie. based on our analysis we select a set of attributes as features and train different machine learning algorithms to predict a movie's expected revenue. based on generated revenue, we classified the movies in 10 categories and achieved a one-class-away accuracy rate of almost 60% (bingo accuracy of 30%). all the generated datasets and analysis codes are available online. we also made the source codes of our scraper bots public, so that researchers interested in extending this work can easily modify these bots as they need and prepare their own up-to-date datasets.",2021-10-13
"prediction of political leanings of chinese speaking twitter users","fenglei gu, duoji jiang","computers and society","this work presents a supervised method for generating a classifier model of the stances held by chinese-speaking politicians and other twitter users. many previous works of political tweets prediction exist on english tweets, but to the best of our knowledge, this is the first work that builds prediction model on chinese political tweets. it firstly collects data by scraping tweets of famous political figure and their related users. it secondly defines the political spectrum in two groups: the group that shows approvals to the chinese communist party and the group that does not. since there are not space between words in chinese to identify the independent words, it then completes segmentation and vectorization by jieba, a chinese segmentation tool. finally, it trains the data collected from political tweets and produce a classification model with high accuracy for understanding users' political stances from their tweets on twitter.",2021-10-12
"inferring offensiveness in images from natural language supervision","patrick schramowski, kristian kersting","computer vision and pattern recognition","probing or fine-tuning (large-scale) pre-trained models results in state-of-the-art performance for many nlp tasks and, more recently, even for computer vision tasks when combined with image data. unfortunately, these approaches also entail severe risks. in particular, large image datasets automatically scraped from the web may contain derogatory terms as categories and offensive images, and may also underrepresent specific classes. consequently, there is an urgent need to carefully document datasets and curate their content. unfortunately, this process is tedious and error-prone. we show that pre-trained transformers themselves provide a methodology for the automated curation of large-scale vision datasets. based on human-annotated examples and the implicit knowledge of a clip based model, we demonstrate that one can select relevant prompts for rating the offensiveness of an image. in addition to e.g. privacy violation and pornographic content previously identified in imagenet, we demonstrate that our approach identifies further inappropriate and potentially offensive content.",2021-10-08
"focus: familiar objects in common and uncommon settings","priyatham kattakinda, soheil feizi","computer vision and pattern recognition","standard training datasets for deep learning often contain objects in common settings (e.g., ""a horse on grass"" or ""a ship in water"") since they are usually collected by randomly scraping the web. uncommon and rare settings (e.g., ""a plane on water"", ""a car in snowy weather"") are thus severely under-represented in the training data. this can lead to an undesirable bias in model predictions towards common settings and create a false sense of accuracy. in this paper, we introduce focus (familiar objects in common and uncommon settings), a dataset for stress-testing the generalization power of deep image classifiers. by leveraging the power of modern search engines, we deliberately gather data containing objects in common and uncommon settings in a wide range of locations, weather conditions, and time of day. we present a detailed analysis of the performance of various popular image classifiers on our dataset and demonstrate a clear drop in performance when classifying images in uncommon settings. by analyzing deep features of these models, we show that such errors can be due to the use of spurious features in model predictions. we believe that our dataset will aid researchers in understanding the inability of deep models to generalize well to uncommon settings and drive future work on improving their distributional robustness.",2021-10-07
"multimodal datasets: misogyny, pornography, and malignant stereotypes","abeba birhane, vinay uday prabhu, emmanuel kahembwe","computers and society","we have now entered the era of trillion parameter machine learning models trained on billion-sized datasets scraped from the internet. the rise of these gargantuan datasets has given rise to formidable bodies of critical work that has called for caution while generating these large datasets. these address concerns surrounding the dubious curation practices used to generate these datasets, the sordid quality of alt-text data available on the world wide web, the problematic content of the commoncrawl dataset often used as a source for training large language models, and the entrenched biases in large-scale visio-linguistic models (such as openai's clip model) trained on opaque datasets (webimagetext). in the backdrop of these specific calls of caution, we examine the recently released laion-400m dataset, which is a clip-filtered dataset of image-alt-text pairs parsed from the common-crawl dataset. we found that the dataset contains, troublesome and explicit images and text pairs of rape, pornography, malign stereotypes, racist and ethnic slurs, and other extremely problematic content. we outline numerous implications, concerns and downstream harms regarding the current state of large scale datasets while raising open questions for various stakeholders including the ai community, regulators, policy makers and data subjects.",2021-10-05
"sensitivity of microwave interferometer in the limiter shadow to filaments in asdex upgrade","mariia usoltceva, stéphane heuraux, ildar khabibullin, helmut faugel, helmut fünfgelder, vladimir bobkov, asdex upgrade team","plasma physics","microwave interferometer in the limiter shadow (mils) is a new diagnostic, installed on asdex upgrade for electron density measurements in the far scrape-off layer (sol). at the chosen frequency of 47 ghz the region of measurements varies within several centimeters before and after the limiter, depending on the density. 200 khz data acquisition allows resolving transient events such as edge localised modes (elms) filaments and turbulence filaments. the measured quantities, phase shift and power decay of the microwave beam, which crosses the plasma, are directly connected to the density and do not depend on any other plasma quantity. in this work, we analyse the influence of a filamentary perturbation on mils signals. simple representation of a filament is adopted, with parameters relevant to experimental filament properties, reported for asdex upgrade. forward modelling is done in comsol software by using raplicasol, to study the response of the mils synthetic diagnostic to the presence of a filament. qualitative and quantitative dependencies are obtained and the boundaries of mils sensitivity to filaments, or to the density perturbation in far sol in general, are outlined.",2021-10-04
"interests, difficulties, sentiments, and tool usages of concurrency developers: a large-scale study on stack overflow","mehdi bagherzadeh, syed ahmed, srilakshmi sripathi, raffi khatchadourian","software engineering","context: software developers are increasingly facing the challenges of writing code that is not only concurrent but also correct. objective: to help these developers, it is necessary to understand concurrency topics they are interested in, their difficulty in finding answers for questions in these topics, their sentiment for these topics, and how they use concurrency tools and techniques to guarantee correctness. method: we conduct a large-scale study on the entirety of stack overflow to understand interests, difficulties, sentiment, and tool usages of concurrency developers. we discuss the implications of our findings for the practice, research, and education of concurrent software development, and investigate the relation of our findings with the findings of the previous work. results: a few findings of our study are: (1) questions that concurrency developers ask can be grouped into a hierarchy with 27 concurrency topics under 8 major categories, (2) thread safety is among the most popular concurrency topics and client-server concurrency is among the least popular, (3) irreproducible behavior is among the most difficult topics and memory consistency is among the least difficult, (4) data scraping is among the most positive concurrency topics and irreproducible behavior is among the most negative, (5) root cause identification has the most number of questions for usage of data race tools and alternative use has the least. conclusion: the results of our study can not only help concurrency developers but also concurrency educators and researchers to better decide where to focus their efforts, by trading off one concurrency topic against another.",2021-09-07
"developing products update-alert system for e-commerce websites users using html data and web scraping technique","ikechukwu onyenwe, ebele onyedinma, chidinma nwafor, obinna agbata","information retrieval","websites are regarded as domains of limitless information which anyone and everyone can access. the new trend of technology put us to change the way we are doing our business. the internet now is fastly becoming a new place for business and the advancement in this technology gave rise to the number of e-commerce websites. this made the lifestyle of marketers/vendors, retailers and consumers (collectively regarded as users in this paper) easy, because it provides easy platforms to sale/order items through the internet. this also requires that the users will have to spend a lot of time and effort to search for the best product deals, products updates and offers on e-commerce websites. they have to filter and compare search results by themselves which takes a lot of time and there are chances of ambiguous results. in this paper, we applied web crawling and scraping methods on an e-commerce website to get html data for identifying products updates based on the current time. the html data is preprocessed to extract details of the products such as name, price, post date and time, etc. to serve as useful information for users.",2021-09-02
"investigating the impacting factors on the public's attitudes towards autonomous vehicles using sentiment analysis from social media data","shengzhao wang, meitang li, bo yu, shan bao, yuren chen","social and information networks","the public's attitudes play a critical role in the acceptance, purchase, use, and research and development of autonomous vehicles (avs). to date, the public's attitudes towards avs were mostly estimated through traditional survey data with high labor costs and a low quantity of samples, which also might be one of the reasons why the influencing factors on the public's attitudes of avs have not been studied from multiple aspects in a comprehensive way yet. to address the issue, this study aims to propose a method by using large-scale social media data to investigate key factors that affect the public's attitudes and acceptance of avs. a total of 954,151 twitter data related to avs and 53 candidate independent variables from seven categories were extracted using the web scraping method. then, sentiment analysis was used to measure the public attitudes towards avs by calculating sentiment scores. random forests algorithm was employed to preliminarily select candidate independent variables according to their importance, while a linear mixed model was performed to explore the impacting factors considering the unobserved heterogeneities caused by the subjectivity level of tweets. the results showed that the overall attitude of the public on avs was slightly optimistic. factors like ""drunk"", ""blind spot"", and ""mobility"" had the largest impacts on public attitudes. in addition, people were more likely to express positive feelings when talking about words such as ""lidar"" and ""tesla"" that relate to high technologies. conversely, factors such as ""covid-19"", ""pedestrian"", ""sleepy"", and ""highway"" were found to have significantly negative effects on the public's attitudes. the findings of this study are beneficial for the development of av technologies, the guidelines for av-related policy formulation, and the public's understanding and acceptance of avs.",2021-08-06
"optimum risk portfolio and eigen portfolio: a comparative analysis using selected stocks from the indian stock market","jaydip sen, sidra mehtab","portfolio management","designing an optimum portfolio that allocates weights to its constituent stocks in a way that achieves the best trade-off between the return and the risk is a challenging research problem. the classical mean-variance theory of portfolio proposed by markowitz is found to perform sub-optimally on the real-world stock market data since the error in estimation for the expected returns adversely affects the performance of the portfolio. this paper presents three approaches to portfolio design, viz, the minimum risk portfolio, the optimum risk portfolio, and the eigen portfolio, for seven important sectors of the indian stock market. the daily historical prices of the stocks are scraped from yahoo finance website from january 1, 2016, to december 31, 2020. three portfolios are built for each of the seven sectors chosen for this study, and the portfolios are analyzed on the training data based on several metrics such as annualized return and risk, weights assigned to the constituent stocks, the correlation heatmaps, and the principal components of the eigen portfolios. finally, the optimum risk portfolios and the eigen portfolios for all sectors are tested on their return over a period of a six-month period. the performances of the portfolios are compared and the portfolio yielding the higher return for each sector is identified.",2021-07-23
"data poisoning won't save you from facial recognition","evani radiya-dixit, sanghyun hong, nicholas carlini, florian tramèr","machine learning","data poisoning has been proposed as a compelling defense against facial recognition models trained on web-scraped pictures. users can perturb images they post online, so that models will misclassify future (unperturbed) pictures. we demonstrate that this strategy provides a false sense of security, as it ignores an inherent asymmetry between the parties: users' pictures are perturbed once and for all before being published (at which point they are scraped) and must thereafter fool all future models -- including models trained adaptively against the users' past attacks, or models that use technologies discovered after the attack. we evaluate two systems for poisoning attacks against large-scale facial recognition, fawkes (500'000+ downloads) and lowkey. we demonstrate how an ""oblivious"" model trainer can simply wait for future developments in computer vision to nullify the protection of pictures collected in the past. we further show that an adversary with black-box access to the attack can (i) train a robust model that resists the perturbations of collected pictures and (ii) detect poisoned pictures uploaded online. we caution that facial recognition poisoning will not admit an ""arms race"" between attackers and defenders. once perturbed pictures are scraped, the attack cannot be changed so any future successful defense irrevocably undermines users' privacy.",2021-06-28
"how well do you know your summarization datasets?","priyam tejaswin, dhruv naik, pengfei liu","computation and language","state-of-the-art summarization systems are trained and evaluated on massive datasets scraped from the web. despite their prevalence, we know very little about the underlying characteristics (data noise, summarization complexity, etc.) of these datasets, and how these affect system performance and the reliability of automatic metrics like rouge. in this study, we manually analyze 600 samples from three popular summarization datasets. our study is driven by a six-class typology which captures different noise types (missing facts, entities) and degrees of summarization difficulty (extractive, abstractive). we follow with a thorough analysis of 27 state-of-the-art summarization models and 5 popular metrics, and report our key insights: (1) datasets have distinct data quality and complexity distributions, which can be traced back to their collection process. (2) the performance of models and reliability of metrics is dependent on sample complexity. (3) faithful summaries often receive low scores because of the poor diversity of references. we release the code, annotated data and model outputs.",2021-06-21
"darknet data mining -- a canadian cyber-crime perspective","edward crowder, jay lansiquot","cryptography and security","exploring the darknet can be a daunting task; this paper explores the application of data mining the darknet within a canadian cybercrime perspective. measuring activity through marketplace analysis and vendor attribution has proven difficult in the past. observing different aspects of the darknet and implementing methods of monitoring and collecting data in the hopes of connecting contributions to the darknet marketplaces to and from canada. the significant findings include a small canadian presence, measured the product categories, and attribution of one cross-marketplace vendor through data visualization. the results were made possible through a multi-stage processing pipeline, including data crawling, scraping, and parsing. the primary future works include enhancing the pipeline to include other media, such as web forums, chatrooms, and emails. applying machine learning models like natural language processing or sentiment analysis could prove beneficial during investigations.",2021-05-18
"volatility modeling of stocks from selected sectors of the indian economy using garch","jaydip sen, sidra mehtab, abhishek dutta","computational finance","volatility clustering is an important characteristic that has a significant effect on the behavior of stock markets. however, designing robust models for accurate prediction of future volatilities of stock prices is a very challenging research problem. we present several volatility models based on generalized autoregressive conditional heteroscedasticity (garch) framework for modeling the volatility of ten stocks listed in the national stock exchange (nse) of india. the stocks are selected from the auto sector and the banking sector of the indian economy, and they have a significant impact on the sectoral index of their respective sectors in the nse. the historical stock price records from jan 1, 2010, to apr 30, 2021, are scraped from the yahoo finance website using the datareader api of the pandas module in the python programming language. the garch modules are built and fine-tuned on the training data and then tested on the out-of-sample data to evaluate the performance of the models. the analysis of the results shows that asymmetric garch models yield more accurate forecasts on the future volatility of stocks.",2021-05-28
"slash or burn: power line and vegetation classification for wildfire prevention","austin park, farzaneh rajabi, ross weber","computer vision and pattern recognition","electric utilities are struggling to manage increasing wildfire risk in a hotter and drier climate. utility transmission and distribution lines regularly ignite destructive fires when they make contact with surrounding vegetation. trimming vegetation to maintain the separation from utility assets is as critical to safety as it is difficult. each utility has tens of thousands of linear miles to manage, poor knowledge of where those assets are located, and no way to prioritize trimming. feature-enhanced convolutional neural networks (cnns) have proven effective in this problem space. histograms of oriented gradients (hog) and hough transforms are used to increase the salience of the linear structures like power lines and poles. data is frequently taken from drone or satellite footage, but google street view offers an even more scalable and lower cost solution. this paper uses $1,320$ images scraped from street view, transfer learning on popular cnns, and feature engineering to place images in one of three classes: (1) no utility systems, (2) utility systems with no overgrown vegetation, or (3) utility systems with overgrown vegetation. the cnn output thus yields a prioritized vegetation management system and creates a geotagged map of utility assets as a byproduct. test set accuracy with reached $80.15\%$ using vgg11 with a trained first layer and classifier, and a model ensemble correctly classified $88.88\%$ of images with risky vegetation overgrowth.",2021-05-09
"documenting large webtext corpora: a case study on the colossal clean crawled corpus","jesse dodge, maarten sap, ana marasović, william agnew, gabriel ilharco, dirk groeneveld, margaret mitchell, matt gardner","computation and language","large language models have led to remarkable progress on many nlp tasks, and researchers are turning to ever-larger text corpora to train them. some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. in this work we provide some of the first documentation for the colossal clean crawled corpus (c4; raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of common crawl. we begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and us military websites. then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark nlp datasets. to understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.",2021-04-18
"combining exogenous and endogenous signals with a semi-supervised co-attention network for early detection of covid-19 fake tweets","rachit bansal, william scott paka, nidhi, shubhashis sengupta, tanmoy chakraborty","computation and language","fake tweets are observed to be ever-increasing, demanding immediate countermeasures to combat their spread. during covid-19, tweets with misinformation should be flagged and neutralized in their early stages to mitigate the damages. most of the existing methods for early detection of fake news assume to have enough propagation information for large labeled tweets -- which may not be an ideal setting for cases like covid-19 where both aspects are largely absent. in this work, we present endemic, a novel early detection model which leverages exogenous and endogenous signals related to tweets, while learning on limited labeled data. we first develop a novel dataset, called ctf for early covid-19 twitter fake news, with additional behavioral test sets to validate early detection. we build a heterogeneous graph with follower-followee, user-tweet, and tweet-retweet connections and train a graph embedding model to aggregate propagation information. graph embeddings and contextual features constitute endogenous, while time-relative web-scraped information constitutes exogenous signals. endemic is trained in a semi-supervised fashion, overcoming the challenge of limited labeled data. we propose a co-attention mechanism to fuse signal representations optimally. experimental results on ectf, politifact, and gossipcop show that endemic is highly reliable in detecting early fake tweets, outperforming nine state-of-the-art methods significantly.",2021-04-12
"counter-strike deathmatch with large-scale behavioural cloning","tim pearce, jun zhu","artificial intelligence","this paper describes an ai agent that plays the popular first-person-shooter (fps) video game `counter-strike; global offensive' (csgo) from pixel input. the agent, a deep neural network, matches the performance of the medium difficulty built-in ai on the deathmatch game mode, whilst adopting a humanlike play style. unlike much prior work in games, no api is available for csgo, so algorithms must train and run in real-time. this limits the quantity of on-policy data that can be generated, precluding many reinforcement learning algorithms. our solution uses behavioural cloning - training on a large noisy dataset scraped from human play on online servers (4 million frames, comparable in size to imagenet), and a smaller dataset of high-quality expert demonstrations. this scale is an order of magnitude larger than prior work on imitation learning in fps games.",2021-04-09
"frozen in time: a joint video and image encoder for end-to-end retrieval","max bain, arsha nagrani, gül varol, andrew zisserman","computer vision and pattern recognition","our objective in this work is video-text retrieval - in particular a joint embedding that enables efficient text-to-video retrieval. the challenges in this area include the design of the visual architecture and the nature of the training data, in that the available large scale video-text training datasets, such as howto100m, are noisy and hence competitive performance is achieved only at scale through large amounts of compute. we address both these challenges in this paper. we propose an end-to-end trainable model that is designed to take advantage of both large-scale image and video captioning datasets. our model is an adaptation and extension of the recent vit and timesformer architectures, and consists of attention in both space and time. the model is flexible and can be trained on both image and video text datasets, either independently or in conjunction. it is trained with a curriculum learning schedule that begins by treating images as 'frozen' snapshots of video, and then gradually learns to attend to increasing temporal context when trained on video datasets. we also provide a new video-text pretraining dataset webvid-2m, comprised of over two million videos with weak captions scraped from the internet. despite training on datasets that are an order of magnitude smaller, we show that this approach yields state-of-the-art results on standard downstream video-retrieval benchmarks including msr-vtt, msvd, didemo and lsmdc.",2021-04-01
"software development during covid-19 pandemic: an analysis of stack overflow and github","pedro almir martins de oliveira, pedro de alcântara dos santos neto, gleison silva, irvayne ibiapina, werney lira, rossana maria de castro andrade","software engineering","the new coronavirus became a severe health issue for the world. this situation has motivated studies of different areas to combat this pandemic. in software engineering, we point out data visualization projects to follow the disease evolution, machine learning to estimate the pandemic behavior, and computer vision processing radiologic images. most of these projects are stored in version control systems, and there are discussions about them in question & answer websites. in this work, we conducted a mining software repository on a large number of questions and projects aiming to find trends that could help researchers and practitioners to fight against the coronavirus. we analyzed 1,190 questions from stack overflow and data science q\&a and 60,352 github projects. we identified a correlation between the questions and projects throughout the pandemic. the main questions about coronavirus are how-to, related to web scraping and data visualization, using python, javascript, and r. the most recurrent github projects are machine learning projects, using javascript, python, and java.",2021-03-09
"msr mining challenge: the smartshark repository mining data","alexander trautsch, fabian trautsch, steffen herbold","software engineering","the smartshark repository mining data is a collection of rich and detailed information about the evolution of software projects. the data is unique in its diversity and contains detailed information about each change, issue tracking data, continuous integration data, as well as pull request and code review data. moreover, the data does not contain only raw data scraped from repositories, but also annotations in form of labels determined through a combination of manual analysis and heuristics, as well as links between the different parts of the data set. the smartshark data set provides a rich source of data that enables us to explore research questions that require data from different sources and/or longitudinal data over time.",2021-02-23
"co-occurrences using fasttext embeddings for word similarity tasks in urdu","usama khalid, aizaz hussain, muhammad umair arshad, waseem shahzad, mirza omer beg","computation and language","urdu is a widely spoken language in south asia. though immoderate literature exists for the urdu language still the data isn't enough to naturally process the language by nlp techniques. very efficient language models exist for the english language, a high resource language, but urdu and other under-resourced languages have been neglected for a long time. to create efficient language models for these languages we must have good word embedding models. for urdu, we can only find word embeddings trained and developed using the skip-gram model. in this paper, we have built a corpus for urdu by scraping and integrating data from various sources and compiled a vocabulary for the urdu language. we also modify fasttext embeddings and n-grams models to enable training them on our built corpus. we have used these trained embeddings for a word similarity task and compared the results with existing techniques.",2021-02-22
"deep learning for suicide and depression identification with unsupervised label correction","ayaan haque, viraaj reddi, tyler giallanza","machine learning","early detection of suicidal ideation in depressed individuals can allow for adequate medical attention and support, which in many cases is life-saving. recent nlp research focuses on classifying, from a given piece of text, if an individual is suicidal or clinically healthy. however, there have been no major attempts to differentiate between depression and suicidal ideation, which is an important clinical challenge. due to the scarce availability of ehr data, suicide notes, or other similar verified sources, web query data has emerged as a promising alternative. online sources, such as reddit, allow for anonymity that prompts honest disclosure of symptoms, making it a plausible source even in a clinical setting. however, these online datasets also result in lower performance, which can be attributed to the inherent noise in web-scraped labels, which necessitates a noise-removal process. thus, we propose sdcnl, a suicide versus depression classification method through a deep learning approach. we utilize online content from reddit to train our algorithm, and to verify and correct noisy labels, we propose a novel unsupervised label correction method which, unlike previous work, does not require prior noise distribution information. our extensive experimentation with multiple deep word embedding models and classifiers display the strong performance of the method in anew, challenging classification application. we make our code and dataset available at this https url",2021-02-18
"numerical turbulence simulations of intermittent fluctuations in the scrape-off layer of magnetized plasmas","gregor decristoforo, audun theodorsen, john omotani, thomas nicholas, odd erik garcia","plasma physics","intermittent fluctuations in the boundary of magnetically confined plasmas are investigated by numerical turbulence simulations of a reduced fluid model describing the evolution of the plasma density and electric drift vorticity in the two-dimensional plane perpendicular to the magnetic field. two different cases are considered, one describing resistive drift waves in the edge region and another including only the interchange instability due to unfavorable magnetic field curvature in the scrape-off layer. analysis of long data time series obtained by single-point recordings are compared to predictions of a stochastic model describing the plasma fluctuations as a super-position of uncorrelated pulses. for both cases investigated, the radial particle density profile in the scrape-off layer is exponential with a radially constant scale length. the probability density function for the particle density fluctuations in the far scrape-off layer has an exponential tail. radial motion of blob-like structures leads to large-amplitude bursts with an exponential distribution of peak amplitudes and the waiting times between them. the average burst shape is well described by a two-sided exponential function. the frequency power spectral density of the particle density is simply that of the average burst shape and is the same for all radial positions in the scrape-off layer. the fluctuation statistics obtained from the numerical simulations are in excellent agreement with recent experimental measurements on magnetically confined plasmas. the statistical framework defines a new validation metric for boundary turbulence simulations.",2021-02-09
"a simple disaster-related knowledge base for intelligent agents","clark emmanuel paulo, arvin ken ramirez, david clarence reducindo, rannie mark mateo, joseph marvin imperial","computation and language","in this paper, we describe our efforts in establishing a simple knowledge base by building a semantic network composed of concepts and word relationships in the context of disasters in the philippines. our primary source of data is a collection of news articles scraped from various philippine news websites. using word embeddings, we extract semantically similar and co-occurring words from an initial seed words list. we arrive at an expanded ontology with a total of 450 word assertions. we let experts from the fields of linguistics, disasters, and weather science evaluate our knowledge base and arrived at an agreeability rate of 64%. we then perform a time-based analysis of the assertions to identify important semantic changes captured by the knowledge base such as the (a) trend of roles played by human entities, (b) memberships of human entities, and (c) common association of disaster-related words. the context-specific knowledge base developed from this study can be adapted by intelligent agents such as chat bots integrated in platforms such as facebook messenger for answering disaster-related queries.",2021-01-25
"an automated pipeline for the vst data log analysis","salvatore savarese, pietro schipani, giulio capasso, mirko colapietro, sergio d'orsi, laurent marty, francesco perrotta","instrumentation and methods for astrophysics","the vst telescope control software logs continuously detailed information about the telescope and instrument operations. commands, telemetries, errors, weather conditions and anything may be relevant for the instrument maintenance and the identification of problem sources is regularly saved. all information are recorded in textual form. these log files are often examined individually by the observatory personnel for specific issues and for tackling problems raised during the night. thus, only a minimal part of the information is normally used for daily maintenance. nevertheless, the analysis of the archived information collected over a long time span can be exploited to reveal useful trends and statistics about the telescope, which would otherwise be overlooked. given the large size of the archive, a manual inspection and handling of the logs is cumbersome. an automated tool with an adequate user interface has been developed to scrape specific entries within the log files, process the data and display it in a comprehensible way. this pipeline has been used to scan the information collected over 5 years of telescope activity.",2020-12-31
"extracting training data from large language models","nicholas carlini, florian tramer, eric wallace, matthew jagielski, ariel herbert-voss, katherine lee, adam roberts, tom brown, dawn song, ulfar erlingsson, alina oprea, colin raffel","cryptography and security","it has become common to publish large (billion parameter) language models that have been trained on private datasets. this paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. we demonstrate our attack on gpt-2, a language model trained on scrapes of the public internet, and are able to extract hundreds of verbatim text sequences from the model's training data. these extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), irc conversations, code, and 128-bit uuids. our attack is possible even though each of the above sequences are included in just one document in the training data. we comprehensively evaluate our extraction attack to understand the factors that contribute to its success. worryingly, we find that larger models are more vulnerable than smaller models. we conclude by drawing lessons and discussing possible safeguards for training large language models.",2020-12-14
